[{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/4-eventparticipated/4.1-event1/","title":"Event 2","tags":[],"description":"","content":"Summary Report: “AWS Well-Architected Security Pillar” Event Objectives Deep dive into the 5 core pillars of AWS Security: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response. Understand modern security principles: Zero Trust, Least Privilege, and Defense in Depth. Learn to automate security checks and incident response using AWS native tools. Identify common cloud threats in the Vietnam market and how to mitigate them. Key Highlights Opening \u0026amp; Security Foundation Core Principles: Moving beyond perimeter security to Defense in Depth, Zero Trust architecture, and strictly enforcing Least Privilege. Shared Responsibility Model: Clarifying what AWS secures (of the cloud) vs. what the customer secures (in the cloud). Local Context: Discussing top security threats specifically targeting cloud environments in Vietnam. Pillar 1: Identity \u0026amp; Access Management (IAM) Modern IAM Architecture: Moving away from long-term credentials (IAM Users) to temporary credentials (IAM Roles). Governance: Using IAM Identity Center for SSO and centralized management. Control: Implementing SCPs (Service Control Policies) and permission boundaries for multi-account environments. Best Practices: Enforcing MFA, regular credential rotation, and using Access Analyzer to validate policies. Pillar 2: Detection \u0026amp; Continuous Monitoring Logging Strategy: \u0026ldquo;Log everything\u0026rdquo; approach using CloudTrail (org-level), VPC Flow Logs, and ALB/S3 logs. Threat Detection: Utilizing Amazon GuardDuty for intelligent threat detection. Centralization: Aggregating findings in AWS Security Hub. Detection-as-Code: Automating alerts using Amazon EventBridge to trigger immediate notifications. Pillar 3: Infrastructure Protection Network Security: Implementing rigorous VPC segmentation and distinguishing strictly between private and public subnets. Firewalls: Understanding the layered defense using WAF (Web Application Firewall), AWS Shield (DDoS), and Network Firewall. Access Control: Differentiating between Stateful (Security Groups) and Stateless (NACLs) firewalls. Pillar 4: Data Protection Encryption Strategy: At-rest: Encrypting S3 buckets, EBS volumes, RDS, and DynamoDB. In-transit: TLS/SSL enforcement. Key Management: Managing keys via AWS KMS (Key Management Service), focusing on grants and rotation policies. Secrets Management: Removing hardcoded credentials from code by using Secrets Manager and Systems Manager Parameter Store. Pillar 5: Incident Response (IR) IR Lifecycle: Preparation -\u0026gt; Detection \u0026amp; Analysis -\u0026gt; Containment, Eradication \u0026amp; Recovery -\u0026gt; Post-Incident Activity. Automation: Using AWS Lambda and Step Functions to auto-remediate issues (e.g., isolating a compromised EC2 instance). Playbooks: Walkthrough of standard responses for scenarios like \u0026ldquo;Compromised IAM Key,\u0026rdquo; \u0026ldquo;S3 Public Exposure,\u0026rdquo; and \u0026ldquo;Malware Detection.\u0026rdquo; Key Takeaways Identity is the New Perimeter Identity management is the most critical line of defense. Long-term access keys are a major risk; utilizing IAM Roles and SSO is mandatory for a modern architecture. Visibility is Paramount You cannot protect what you cannot see. Enabling centralized logging (CloudTrail, Config) and threat detection (GuardDuty) is the first step in any security strategy. Automate Security Humans are slow; attacks are fast. Security responses (locking down a user, blocking an IP) should be automated via code (Lambda/EventBridge) wherever possible. Applying to Work Audit IAM Policies: Review current permissions to ensure \u0026ldquo;Least Privilege\u0026rdquo; and remove unused IAM Users. Enable GuardDuty: Activate GuardDuty in the main region to detect anomalies immediately. Encrypt Data: Ensure all new S3 buckets and EBS volumes have default encryption enabled via KMS. Develop IR Playbooks: Draft a basic Incident Response plan for \u0026ldquo;S3 Public Leak\u0026rdquo; and \u0026ldquo;Compromised Credentials\u0026rdquo; scenarios. Event Experience The \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; workshop was an intensive and highly focused morning session. It provided a structured approach to security that is often overlooked in rushed development cycles.\nDeep Dive into \u0026ldquo;Defense in Depth\u0026rdquo; The session on Infrastructure Protection clarified how to layer security controls (WAF -\u0026gt; NACL -\u0026gt; SG) so that if one fails, others are still in place. Practical Focus on Automation Seeing the \u0026ldquo;Mini Demo\u0026rdquo; on validating IAM policies and simulating access was very helpful. It showed that security can be tested just like software code. The Incident Response segment changed my perspective: instead of waking up at 3 AM to fix a hack manually, we can write Lambda functions to contain threats automatically. Local Context Hearing about common pitfalls in Vietnamese enterprises helped me relate the theoretical concepts to the actual risks our business faces daily. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/4-eventparticipated/4.2-event2/","title":"Event 1","tags":[],"description":"","content":"Summary Report: “DevOps on AWS” Event Objectives Understand the DevOps mindset, culture, and key performance metrics (DORA). Master the AWS CI/CD toolchain for automated software delivery. Learn Infrastructure as Code (IaC) principles using CloudFormation and CDK. Explore containerization strategies and observability best practices on AWS. Key Highlights DevOps Culture and Key Metrics Mindset Shift: Moving away from silos to shared responsibility between Development and Operations. Key Metrics (DORA): Focusing on Deployment Frequency, Lead Time for Changes, Mean Time to Restore (MTTR), and Change Failure Rate to measure success. Benefits: Faster innovation, higher reliability, and improved collaboration. AWS DevOps Services – CI/CD Pipeline We explored the full automation pipeline, moving from manual deployments to orchestration:\nSource Control: Utilizing AWS CodeCommit and implementing Git strategies like GitFlow and Trunk-based development. Build \u0026amp; Test: Using AWS CodeBuild for compiling source code, running tests, and producing software packages. Deployment Strategies: implementing AWS CodeDeploy for Blue/Green, Canary, and Rolling updates to minimize downtime. Orchestration: Tying it all together with AWS CodePipeline. Infrastructure as Code (IaC) Transitioning from manual console clicks to code-defined infrastructure:\nAWS CloudFormation: Using JSON/YAML templates to define resources, manage stacks, and detect infrastructure drift. AWS CDK (Cloud Development Kit): Defining cloud resources using familiar programming languages (Python, TypeScript, Java) to create reusable constructs. Tool Selection: Discussing when to use declarative templates (CloudFormation) vs. imperative code (CDK). Container Services on AWS Docker Fundamentals: Packaging applications into lightweight, portable containers. Registry: Using Amazon ECR for secure image storage and vulnerability scanning. Orchestration: Comparing Amazon ECS ( "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"How Volkswagen and AWS Built a Complete MLOps Process for the Digital Production Platform By: Gabriel Zylka, Chandana Keswarkar, and Sandro Zangiacomi Date: March 03, 2025\nKeywords: Amazon API Gateway, Amazon DataZone, Amazon EventBridge, Amazon SageMaker, Amazon Simple Storage Service (S3), Automotive, AWS CodeArtifact, AWS CodeCommit, AWS Lake Formation, AWS Service Catalog, AWS Step Functions, AWS Well-Architected Framework, Industries\nBackground In 2019, Volkswagen AG (VW) and Amazon Web Services (AWS) formed a strategic collaboration to develop the Digital Production Platform (DPP). This strategy was designed to improve VW\u0026rsquo;s manufacturing and logistics efficiency by 30% while simultaneously reducing production costs by a similar margin. The DPP facilitates access to data from VW\u0026rsquo;s manufacturing devices and systems, making the building and deployment of new applications 2-3 times simpler and faster, while reducing testing costs and facilitating the sharing of solutions across VW businesses. A key solution adopted was a complete MLOps process for Machine Learning (ML) related problems. This article presents the architecture used to standardize the entire lifecycle of an ML project, best practices, and how to implement a similar MLOps solution in your organization.\nThe VW team has deployed over 100 use cases across VW companies and brands. The majority of these are ML-based solutions in areas such as predictive maintenance, quality control, and process optimization. For example, predictive maintenance for welding gun robots involves sensors detecting motor or welding circuit faults. These ML models are required to predict these faults early from thousands of robots across multiple different factories. Many data science teams worked to create operation-ready ML solutions, complying with corporate security standards. However, this decentralized approach revealed several issues:\nInconsistent Development: Each factory developed solutions independently, leading to fragmented operational results and the lack of a unified strategy. This created a collection of disjointed and disconnected solutions. Inefficient Development: Redundant efforts arose from VW teams having to recreate similar infrastructure components, with each component requiring a different security assessment, increasing complexity. Impact on Time and Resources: Initial deployments required 2 full-time employees working for 2 months per workstream. Training new members and completing security assessments also took longer because each location had its own implementation method. Process Management Issues: Without a standard process, VW teams struggled to manage, trace, and version control models. This affected transparency. Maintenance and Quality Challenges: Diverse implementations led to uneven quality, wasted resources, impacted testing standards, and made it difficult to adopt best practices. These issues resulted in financial impacts, slowed time-to-market, increased maintenance costs, and created additional security risks. Knowledge sharing became difficult, leading to wasted effort and resources on non-differentiating tasks, adding to the burden of operations and maintenance. To address these issues, VW partnered with AWS Professional Services to build a more secure, scalable MLOps solution for enterprises using ML to deploy on the DPP.\nMLOps Architecture The architecture implemented at VW demonstrated how MLOps automates every stage, creating an efficient process to manage the entire machine learning model lifecycle. For a deeper dive, you can read the article MLOps foundation roadmap for enterprises with Amazon SageMaker.\nA multi-account strategy helps manage multiple models. Here is how each account functions:\nData Account: This account acts as a hub for data management, monitoring all data ingestion processes from sources like on-premises systems or other cloud environments. Administrators control and restrict access to specific data columns to meet use case requirements, ensuring compliance through anonymization when necessary. To learn more about how VW manages access and data security using Amazon DataZone, refer to the blog post. EXP Account (Experimentation): This account provides a dedicated environment for VW\u0026rsquo;s data science team to perform data exploration, experimentation, and model training. The EXP account deploys all resources within an isolated VPC without internet access. To use third-party libraries, an AWS CodeArtifact repository provides secure access to public repositories like PyPI. Data Scientists commit code changes to an AWS CodeCommit repository (or other providers like GitLab, as AWS CodeCommit access for new users has ended). When training or inference requires custom images, Data Scientists commit code that is pushed to a repository. Then, a CI/CD pipeline scans, tests, and builds the images before pushing them to a central Amazon ECR registry in the RES account. RES Account (Resources): This account manages all infrastructure and ML model deployments. It houses the CodeCommit repositories for Infrastructure as Code (IaC) and AWS CodePipeline CI/CD workflows, enabling deployment across all RES, EXP, DEV, INT, and PROD accounts. Additionally, this account hosts Amazon ECR repositories where scientists publish Docker containers with custom images used during model inference. Finally, it creates AWS Service Catalog products in the EXP account to execute model training pipelines and model deployment pipelines in the RES account. DEV Account (Development): This account serves as the development environment where VW teams initially deploy ML models to Amazon SageMaker endpoints. Here, models undergo complete testing by VW for all model metrics such as performance and infrastructure parameters like response time and availability. In the DEV account, administrators typically grant manual access to data scientists and DevOps teams to test and troubleshoot deployments. Once testing is complete, a manual approval step in the CI/CD pipeline in the RES account promotes the deployment to the INT stage. INT Account (Integration): This account acts as a staging environment for ML model deployment to validate that the deployment and infrastructure integration are successful before proceeding to the PROD environment. Unlike the DEV environment, deployments in the INT account can generally only be accessed via read-only permissions. After passing all testing procedures, the DevOps team approves via the CI/CD pipeline in the RES account to deploy the model to production. PROD Account (Production): This account hosts the ML model version on an Amazon SageMaker endpoint. In the production environment, you can configure the SageMaker endpoint with an auto scaling group to automatically scale the endpoint up or down based on demand. MLOps for Data Scientists The machine learning lifecycle is an iterative process, starting with defining a business problem and deciding if ML is an applicable solution. Once confirmed, the process involves defining the ML problem, followed by the data phase, where Data Engineers collect, explore, prepare, and analyze data through visualization. Next is Feature Engineering, involving specific techniques like encoding, normalization, and handling missing data. Then comes the model development phase, which includes selecting a suitable algorithm, training the model, hyperparameter tuning, and evaluating performance using predefined metrics. Once the model meets the desired performance criteria, it is deployed to the production environment. Over time, model performance may degrade, requiring continuous monitoring, debugging, retraining, and redeployment to maintain effectiveness. The following steps describe the workflow of a Data Scientist for MLOps solutions across different accounts:\nData Collection and Preparation: Data Engineers create Extract, Transform, and Load (ETL) pipelines combining multiple data sources and prepare the necessary datasets for ML problems in the DATA account. Data is cataloged using AWS Glue Data Catalog and shared with other users and accounts via AWS Lake Formation for governance. Data Scientists are granted secure access to specific datasets from the DATA account. Data Exploration and Model Development: Each Data Scientist receives an Amazon SageMaker Studio user profile with an IAM role and Security Group to access SageMaker Studio and their specific datasets in Amazon S3. In their personal workspace, Data Scientists perform tasks such as data exploration, model training, hyperparameter tuning, data processing, and model evaluation using Jupyter Notebooks or SageMaker services. This can be extended with Amazon SageMaker Feature Store for reuse. For more information, you can refer to Enable feature reuse across accounts and teams using Amazon SageMaker Feature Store. Model Training and Retraining: After the experimentation phase, the Data Scientist launches the \u0026ldquo;Model Building Product\u0026rdquo; from AWS Service Catalog. This initiates a CloudFormation stack to set up a SageMaker Pipeline to orchestrate data tasks such as processing, training, and evaluation. Successfully trained and evaluated ML models are registered in the SageMaker Model Registry, which stores version history and deployment metadata, such as container images and artifact locations. For subsequent retraining, Data Scientists trigger the Training Pipeline, which registers a new model version into the Model Registry upon successful execution. When a new model version is registered, an Amazon EventBridge event is triggered and sent to the RES account, initiating the deployment process. Model Deployment and Redeployment: To create a model deployment pipeline, the DevOps engineer launches the \u0026ldquo;Model Deployment\u0026rdquo; product from Service Catalog, referencing the trained model in the Model Registry. This product provisions a CodeCommit repository for IaC, a CodePipeline, and an EventBridge rule to listen for new model versions from the EXP account. The CI/CD pipeline is triggered by changes in the CodeCommit repository and by events coming from EventBridge. It queries the Model Registry to retrieve the latest version and deploys the model along with related resources to the DEV stage. After manual approval steps, the model continues to be promoted through the INT and PROD stages. Benefits This new MLOps process brings several key benefits:\nStandardization: By replacing multiple custom solutions with a unified common process, VW eliminated redundant work and established consistent practices across all ML activities. Operational Efficiency: A structured account architecture divided into different environments clarifies responsibilities and optimizes the entire ML lifecycle from experimentation to deployment. Security and Governance: Built-in security guardrails, including dedicated IAM roles, isolated VPCs, and encryption, ensure ML activities meet enterprise security standards while maintaining flexibility. Scalability: The solution currently supports 8 projects across 5 factories, serving 16 Data Scientists, with an architecture designed to easily scale in the future and accommodate more new projects. Reduced Time to Market: An automated and standardized process can now complete work in just a few days, whereas it previously took 2 full-time employees 2 months for a single project. This significantly accelerates model deployment. Open Source Repository A Github repository provides a solution template to deploy the MLOps infrastructure discussed in this blog post. This solution deploys a total of 13 configurable AWS CDK stacks across 5 AWS accounts, allowing you to quickly bootstrap an MLOps platform. The template is flexible and can be customized to meet your specific requirements, such as adding Service Catalog Products to refine training and deployment pipelines. To perform the deployment, you will need access to 5 AWS accounts for the following environments:\nEXP (Experimentation) RES (Resources) DEV (Development) INT (Integration) PROD (Production) For prerequisites and deployment instructions, please refer to the README.md file in the repository.\nPossible Extensions The following extensions can enhance the MLOps solution to meet specific use case needs:\nBatch Processing While online inference provides low-latency real-time predictions, batch inference is ideal for scenarios where data arrives in batches at regular intervals and immediate results are not required. Batch inference is particularly suitable for periodic inference needs. By using Amazon SageMaker, you can perform batch inference by running a batch transform job on a SageMaker Model or orchestrating a batch inference workflow with AWS Step Functions. Results are automatically stored in Amazon S3.\nAPI Gateway To serve models via custom API endpoints, use Amazon API Gateway, a fully managed service to create, publish, maintain, monitor, and secure APIs at any scale. Requests are routed through API Gateway to AWS Lambda, which invokes the SageMaker endpoint and returns responses to API Gateway for testing and serving predictions.\nModel Monitoring Amazon SageMaker Model Monitor enables continuous monitoring of model performance after deployment in the production environment. This tool collects input data samples and model predictions at scheduled intervals, monitoring metrics such as data quality, model quality, bias, and explainability. If Model Monitor detects any deviation or degradation, it generates alerts, allowing you to take corrective actions such as collecting new training data, retraining the model, or auditing upstream systems. Learn more about Monitoring in-production ML models at large scale using Amazon SageMaker Model Monitor.\nSecurity Guardrails VW\u0026rsquo;s MLOps solution follows AWS security best practices from the AWS Well-Architected Framework Security pillar and the AWS whitepaper Build a Secure Enterprise Machine Learning Platform on AWS. In addition to security features implemented in each VW AWS account, the following best practices were observed:\nInfrastructure Protection: All resources are segregated within self-managed VPC subnets. All AWS services are accessed via VPC Service Endpoints. Data Protection: All stored data is encrypted using customer-managed encryption keys. SageMaker Studio environments are encrypted at rest. Identity and Access Management: Dedicated IAM roles are assigned to SageMaker Studio users, pipelines, training jobs, and model endpoints. IAM roles are created using Amazon SageMaker Role Manager personas to enforce least privilege access. Explicit IAM Deny policies to restrict model training or model deployment outside the VPC or without encryption. Auditing: Regular security audits are performed to identify and mitigate vulnerabilities. AWS Config is used to track configuration changes and ensure compliance with security policies. AWS Security Hub aggregates security findings from multiple AWS services for centralized management and remediation. Learn more about VW secures landing zone with automated remediation of security findings. Conclusion The collaboration between VW and AWS successfully transformed a fragmented MLOps landscape into a standardized, efficient, and more secure ML production process. By implementing a comprehensive MLOps solution built on Amazon SageMaker, VW addressed the challenges of decentralized development to establish a streamlined, scalable, and secure ML lifecycle through a multi-account MLOps architecture. This implementation can serve as a blueprint for other enterprises looking to standardize their MLOps operations at scale. If you are interested in exploring similar solutions or need guidance on building your own MLOps solution, visit the AWS for automotive page or contact your AWS team today.\nAuthors Gabriel Zylka: An ML Engineer at AWS Professional Services. He works closely with customers to accelerate their cloud transformation journey. Specializing in MLOps, he focuses on productizing ML workloads by automating the end-to-end ML lifecycle and helping achieve desired business outcomes. Chandana Keswarkar: A Senior Solution Architect at AWS, specializing in guiding automotive customers on their digital transformation journey using cloud technology. She helps organizations develop and refine platform and product architectures while making informed design decisions. In her free time, she enjoys traveling, reading, and practicing yoga. Sandro Zangiacomi: An AWS Professional Services AI expert based in Paris. In his current role, he assists customers in orchestrating machine learning workflows and building machine learning platforms for various use cases, including GenAI implementation. In his free time, he enjoys spending quality moments with friends in Paris and learning about almost everything, from personal development to fashion. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Implement recovery testing to validate recovery with AWS Backup By Gabe Contreras and Sabith Venkitachalapathy | March 28, 2025 Category: Advanced (300), AWS Backup, Storage, Technical\nCritical applications are the foundation for everything from e-commerce to healthcare, making a solid backup strategy not just a best practice but a necessity. Threats like ransomware are becoming increasingly sophisticated, and for AWS users, backups alone are not enough. Organizations need confidence that these protections will work effectively when disaster strikes.\nManual testing, while necessary, consumes Information Technology resources. Through scheduled automated recovery testing, organizations achieve more than just efficiency: they establish a system that verifies backup integrity, streamlines compliance reporting, and frees up valuable human resources—all while building certainty that their protection strategy will deliver results when needed.\nIn our previous post, Validating recovery readiness with AWS Backup restore testing, we explored why AWS Backup testing and recovery are crucial for meeting internal Disaster Recovery (DR) policies and regulatory requirements. In a digital landscape shaped by regulations like the European Union’s Digital Operational Resilience Act (DORA) and the New York Department of Financial Services (NYDFS), resilience is not just a goal but a mandatory requirement. AWS Backup restore testing can provide the evidence required by these regulations; meanwhile, testing and auditing features offer capabilities to help organizations validate and report on their resilience enhancement efforts.\nIn this article, you will learn how to configure AWS Backup restore testing and some best practices to consider when creating your own plan. You will also get an example to see how end-to-end recovery testing works in practice.\nHow AWS Backup restore testing works AWS Backup restore testing allows users to test data recovery on a predetermined schedule and validate the restored data. The ability to set schedules and create automated processes to check data recovery helps reduce manual effort and meet compliance requirements.\nWithout automated recovery testing, personnel might have to select systems and recovery points, complete the restore manually, and ask application teams to validate the restored data. This consumes time and resources across multiple teams, which could be used more effectively to improve applications. By automating this process, we can create data validation workflows to validate restores on a regular basis.\nAn AWS Backup restore testing plan is built in two stages:\nFirst, you create the restore testing plan. Second, you create the protected resource selections that will be restored by that testing plan. When AWS Backup completes a restore, you can build a validation process for the restore test using AWS Lambda functions triggered by Amazon EventBridge. Lambda functions can perform various validation activities—including checking connectivity, retrieving objects from Amazon S3, or getting the status of encryption keys to validate actual data—then report back to AWS Backup whether that validation was successful or failed. After the restore test is complete, you can use AWS Backup Audit Manager reports to demonstrate compliance when needed.\nBuilding a restore testing plan The first step of implementing recovery testing is building the restore testing plan. Since there are many factors to consider regarding test frequency and what to test, we will focus on best practices and recommendations. A restore testing plan consists of three parts:\nTest frequency Start within time Recovery point selection criteria For Test frequency, you should test critical resources daily or weekly. You should test resources within their retention period, meaning if we keep recovery points for 14 days, you should run tests more frequently than that.\nThe Start within time depends on the number of recovery points you will test and how long each restore takes to complete. Each service has a maximum concurrent restores limit allowed, and you need to ensure your plans are spaced out so as not to exceed that limit. Recovery point selection can include all or specific vaults, the time frame for eligible recovery points, and whether to include Point-In-Time Recovery (PITR) resource types. You might have one vault per account, or multiple vaults based on application types or tiers. If you are replicating data to a central backup account, an optimal design would be to create a logically air-gapped (LAG) vault for each source account. After creating the restore testing plan, you move to stage 2: creating protected resource selections. For each resource selection, you must choose a single resource type, such as Amazon S3 or Amazon Relational Database Service (Amazon RDS). After selecting a resource type, restore testing allows you to further customize which specific resources will be selected. Each restore testing plan allows up to 30 protected resource selections. When creating the resource assignment, you can select the default AWS Backup IAM role. If the default role does not exist, it will be created with the appropriate permissions. You can filter resources by individual selection or by tags, which allows you to select specific resources to test based on your requirements. In Figure 4, we select S3 as the resource type, then filter by tag to select specific buckets.\nEach service has its own set of possible recovery metadata, providing default values to perform a successful restore test, where AWS Backup will infer a minimum set of recovery metadata. There is also overrideable metadata that you can change to override the default values. You can read more about inferred and overrideable metadata in the AWS Backup documentation. After defining the overall plan and resource selection, we have a fully operational plan as shown in Figure 5. Implementing restore validation Configuring a restore testing plan is only half the work; you also have to verify that your restored data is usable. AWS Backup sends Amazon EventBridge events for changes in restore job status. We can use these events to trigger an AWS Lambda function when a restore test job transitions to a completed state, allowing application teams to create code to test their data. The testing code depends on the service you are protecting but could include retrieving objects from an S3 bucket or querying Amazon DynamoDB. Once your Lambda function finishes running, it can report back to AWS Backup regarding success or failure. Figure 6 illustrates this sample restore validation workflow. If you have multiple restore testing plans, you can customize the EventBridge rule to send certain events to corresponding functions (as illustrated in Figure 7) by including the Amazon Resource Name (ARN) of the restore testing plan. Using the restore testing plan ARN also allows you to filter out manual restores. After creating the event pattern, you select a target to send the event to. If you have multiple resource types requiring distinct testing criteria, an orchestrator Lambda function can help send events for different resource types to the correct validation process. This orchestrator Lambda function will check the resource type and route the event to the corresponding data validation Lambda function. If you have multiple protected resource types, you would select this orchestrator Lambda function as the target for EventBridge events, as seen in Figure 8. Here is sample code for the Restore Orchestrator Lambda:\nimport json import boto3 import logging logger = logging.getLogger() logger.setLevel(logging.INFO) lambda_client = boto3.client(\u0026#39;lambda\u0026#39;) def handler(event, context): logger.info(\u0026#34;Handling event: %s\u0026#34;, json.dumps(event)) resource_type = event.get(\u0026#39;detail\u0026#39;, {}).get(\u0026#39;resourceType\u0026#39;, \u0026#39;\u0026#39;) function_name = None try: if resource_type == \u0026#34;RDS\u0026#34;: function_name = \u0026#34;RDSRestoreValidation\u0026#34; logger.info(\u0026#34;Resource is an RDS instance. Invoking Lambda function: %s\u0026#34;, function_name) elif resource_type == \u0026#34;S3\u0026#34;: function_name = \u0026#34;S3RestoreValidation\u0026#34; logger.info(\u0026#34;Resource is an S3 bucket. Invoking Lambda function: %s\u0026#34;, function_name) else: raise ValueError(f\u0026#34;Unsupported resource type: {resource_type}\u0026#34;) # Invoke the appropriate Lambda function response = lambda_client.invoke( FunctionName=function_name, Payload=json.dumps(event), InvocationType=\u0026#34;RequestResponse\u0026#34; ) logger.info(\u0026#34;Lambda invoke response: %s\u0026#34;, response) except Exception as e: logger.error(\u0026#34;Error during Lambda invocation: %s\u0026#34;, str(e)) raise e logger.info(\u0026#34;Finished processing event for resource type: %s\u0026#34;, resource_type) In the code of the recovery orchestration Lambda function, you can see that if the resource type matches S3, it forwards the entire event to another Lambda function called S3RestoreValidation. This S3RestoreValidation function then performs the validation of the restore on an S3 resource and reports back to AWS Backup whether the validation was successful or failed.\nimport json import boto3 import logging logger = logging.getLogger() logger.setLevel(logging.INFO) s3_client = boto3.client(\u0026#39;s3\u0026#39;) backup_client = boto3.client(\u0026#39;backup\u0026#39;) def handler(event, context): logger.info(\u0026#34;Handling event: %s\u0026#34;, json.dumps(event)) restore_job_id = event.get(\u0026#39;detail\u0026#39;, {}).get(\u0026#39;restoreJobId\u0026#39;, \u0026#39;\u0026#39;) resource_type = event.get(\u0026#39;detail\u0026#39;, {}).get(\u0026#39;resourceType\u0026#39;, \u0026#39;\u0026#39;) created_resource_arn = event.get(\u0026#39;detail\u0026#39;, {}).get(\u0026#39;createdResourceArn\u0026#39;, \u0026#39;\u0026#39;) validation_status = \u0026#34;SUCCESSFUL\u0026#34; validation_status_message = \u0026#34;Restore validation completed successfully\u0026#34; try: if resource_type == \u0026#34;S3\u0026#34;: bucket_name = get_bucket_name_from_arn(created_resource_arn) # List objects in the bucket response = s3_client.list_objects_v2(Bucket=bucket_name) # Check if the bucket contains more than 1 object object_count = response.get(\u0026#39;KeyCount\u0026#39;, 0) if object_count \u0026gt; 1: logger.info(f\u0026#34;Bucket {bucket_name} contains more than 1 object. Validation successful.\u0026#34;) else: logger.info(f\u0026#34;Bucket {bucket_name} contains 1 or fewer objects. Validation failed.\u0026#34;) validation_status = \u0026#34;FAILED\u0026#34; validation_status_message = f\u0026#34;Bucket {bucket_name} contains only {object_count} object(s).\u0026#34; else: validation_status = \u0026#34;FAILED\u0026#34; validation_status_message = f\u0026#34;Unsupported resource type: {resource_type}\u0026#34; # Report validation result to AWS Backup backup_client.put_restore_validation_result( RestoreJobId=restore_job_id, ValidationStatus=validation_status, ValidationStatusMessage=validation_status_message ) logger.info(\u0026#34;Restore validation result sent successfully\u0026#34;) except Exception as e: logger.error(\u0026#34;Error during restore validation: %s\u0026#34;, str(e)) validation_status = \u0026#34;FAILED\u0026#34; validation_status_message = f\u0026#34;Restore validation encountered an error: {str(e)}\u0026#34; # Report failure result to AWS Backup backup_client.put_restore_validation_result( RestoreJobId=restore_job_id, ValidationStatus=validation_status, ValidationStatusMessage=validation_status_message ) logger.info(\u0026#34;Finished processing restore validation for job ID: %s\u0026#34;, restore_job_id) def get_bucket_name_from_arn(arn): arn_parts = arn.split(\u0026#34;:\u0026#34;) resource_parts = arn_parts[-1].split(\u0026#34;/\u0026#34;) return resource_parts[-1] The S3RestoreValidation code validates the S3 restore by verifying that the bucket contains more than one object. After the check, it reports back to AWS Backup whether the restore completed successfully. A fully successful restore and validation will result in a summary like Figure 9. The job status will be Completed, and the validation status will be Successful. When setting the validation status in your Lambda code, you can optionally add a validation message, which will appear in the console and AWS Backup APIs. You can read more about restore validation and examples in the documentation.\nAWS Backup automatically initiates the deletion of the restored resources once the validation is submitted or when the cleanup period expires. Deletion times can vary depending on the resource type. Most resources are deleted quickly, but some may take longer. For example, deleting an S3 bucket is a two-step process: first adding lifecycle rules to delete objects, and then deleting the bucket when it is empty. These lifecycle rules can take a few days to execute.\nConsiderations for AWS Backup restore testing Now that you understand the best practices for creating restore testing and validation plans, there are still some implementation details you should consider.\nCost Optimization Cost optimization plays a critical role throughout the backup lifecycle, including recovery testing. Here is how to manage costs effectively:\nSelect resources wisely: Use tags or selections to test only critical resources, avoiding non-production resources unless compliance requires it. Schedule tests by criticality: Schedule tests according to criticality (daily or weekly for critical resources, quarterly or semi-annually for others) aligned with policies and retention periods (e.g., test within 14 days if the retention period is 14 days). Optimize retention time: Minimize data recovery time to reduce costs. Set deletion times based on automated tests. Reference pricing for restore testing can be found on the AWS Backup pricing page.\nAWS Backup Audit and Reporting AWS Backup Audit Manager helps you ensure your backup policies and resources comply with internal standards or regulations. This tool tracks whether resources are backed up, backup frequency, whether vaults are logically isolated, and whether recovery times meet objectives. AWS Backup Audit Manager audit frameworks allow this by providing pre-built controls or custom options to ensure resource policy compliance.\nAudit reports provide evidence of compliance that can be shared. There are two types of reports:\nJob Reports: Show completed and active jobs in the last 24 hours (e.g., restore job reports for recent restores). Compliance Reports: Monitor resource status or framework controls. Management accounts gain visibility across multiple accounts to generate organization-wide reports. See the AWS Backup documentation for steps to create reports and details on using frameworks.\nRestore testing plan scale When creating restore testing plans, ensure that your plans meet testing requirements and complete on time. Each resource type has a limit on the number of concurrent restore jobs from testing plans (not on-demand restores).\nThe \u0026ldquo;Start within\u0026rdquo; window from stage 1 is a key factor. The \u0026ldquo;Start within\u0026rdquo; configuration means that all resource selections for a restore testing plan must start within this window, and you need to be careful not to exceed concurrency limits.\nExample: Amazon S3 allows 30 concurrent restores, so selecting 90 buckets with a one-hour window risks causing delays.\nTo plan effectively, use a longer \u0026ldquo;start within\u0026rdquo; window or create multiple plans with staggered start times—especially when frequent (daily/weekly) and periodic (monthly/quarterly) tests run simultaneously. Check adjustable limits in the documentation and request limit increases if needed.\nVisualizing the complete testing process To see how comprehensive recovery testing works and how to implement and integrate testing plans, we have included a sample restore testing plan. This sample plan helps you visualize each step of the process and see how restore and validation interact.\nThis is a pre-configured AWS CloudFormation stack that runs automatically on a daily schedule.\nPrerequisites The following prerequisites are required to complete this solution:\nAWS Backup configured in your account. Amazon S3 and/or Amazon RDS recovery points. Launch AWS CloudFormation stack This AWS CloudFormation template deploys everything needed to automatically test recovery for both Amazon S3 and Amazon RDS.\nhttps://awsstorageblogresources.s3.us-west-2.amazonaws.com/blog1418/CFAWSBackupRestoreTestingV15.yaml\nRun the restore testing plan Once deployed, no manual intervention is needed to run the plan. The restore testing plan runs once daily on all resources selected by that plan. As noted in Figure 6, AWS Backup completes the restore process, then runs Lambda functions to validate the restores.\nWhen the validation process completes successfully, the validation will appear as shown in Figure 10. Cleanup All resources restored from the restore testing plan will be automatically deleted after four hours. If you are using restore testing for Amazon S3 resources, deleting S3 buckets containing data will take longer. This is because lifecycle rules take a few days to execute. To avoid incurring additional costs, delete the CloudFormation stack, which will remove the restore testing plans and stop future tests. For instructions, refer to Deleting a stack on the CloudFormation console.\nConclusion AWS Backup restore testing is a versatile and scalable feature that allows you to tailor a solution to your organization\u0026rsquo;s needs. You can start by understanding your organizational policies, then explore AWS Backup restore testing capabilities in the AWS Management Console and learn how to integrate automated testing into your Disaster Recovery (DR) and cyber resilience strategies. To implement AWS Backup restore testing in your environment, visit the AWS Backup documentation. You can also work with AWS Solutions Architects to design a comprehensive backup validation strategy tailored to your organization\u0026rsquo;s needs.\nThe message is clear: automated backup validation is no longer an optional choice; it is a fundamental requirement to ensure business continuity in the modern era. Regular testing helps meet internal policies, regulatory requirements, and cyber resilience mandates, while AWS Backup restore testing provides an efficient, scalable solution to ensure recovery readiness.\nAuthor Information Gabe Contreras Gabe Contreras is a Senior Storage Solutions Architect for the Strategic Accounts division. He always looks forward to deep discussions with customers to find the best solutions for their needs. He enjoys understanding how things operate and solving complex problems.\nSabith Venkitachalapathy Sabith Venkitachalapathy is an expert in designing AWS recovery solutions, ensuring disaster recovery and high availability for critical workloads. Focusing on Financial Services (FSI) and Healthcare \u0026amp; Life Sciences (HCLS), Sabith leverages AWS to solve industry challenges and drive innovation. He shares practical insights to help organizations build secure and resilient cloud architectures.\n"},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Improve PostgreSQL Performance using pgstattuple extension by Vivek Singh, Kiran Singh, and Sagar Patel | March 02, 2025 | at Advanced (300), Amazon Aurora, Amazon RDS, PostgreSQL compatible, RDS for PostgreSQL, Technical How-to\nAs businesses continue to generate and store massive amounts of data, the need for efficient and reliable database management systems becomes increasingly critical. PostgreSQL, an open-source relational database management system (RDBMS), has established itself as a powerful solution for handling complex data requirements. One of PostgreSQL\u0026rsquo;s key strengths lies in its extensibility. Through a rich ecosystem of extensions and plugins, developers can enhance the database\u0026rsquo;s functionality to meet specific requirements. These extensions range from spatial data support and full-text search capabilities to advanced data types and performance optimization tools. While PostgreSQL offers a wide array of features and capabilities, one often overlooked extension is pgstattuple—a tool that can provide significant value in gaining insights into the inner workings of a PostgreSQL database.\nIn this post, we dive deep into pgstattuple—what insights it provides, how to use it to diagnose issues in Amazon Aurora PostgreSQL-Compatible Edition and Amazon Relational Database Service (Amazon RDS) for PostgreSQL, and best practices for harnessing its capabilities.\nOverview of pgstattuple The pgstattuple extension provides a set of functions to query detailed statistics at the tuple (record) level in PostgreSQL tables and indexes. This allows for a deep look into the physical storage layer that standard PostgreSQL statistical views cannot provide.\nSome table and index level metrics that pgstattuple provides include:\ntuple_count – Number of live tuples dead_tuple_count – Number of dead tuples not yet cleaned up tuple_len – Average length of live tuples in bytes free_space – Total free space available in bytes free_percent – Percentage of free space; higher values indicate more bloat dead_tuple_len – Total length of dead tuples in bytes dead_tuple_percent – Percentage of space occupied by dead tuples These metrics are not just mere numbers—they are an early warning system for database health and performance issues. By monitoring these statistics, you can proactively identify storage issues that might be silently affecting your database performance. Whether it is excessive table bloat consuming disk space, or index fragmentation slowing down queries, pgstattuple helps detect these problems before they become critical incidents.\nUsing pgstattuple in Aurora and Amazon RDS Both Aurora and Amazon RDS support the use of the pgstattuple extension. To enable it, you first need to create the extension in your database using the command CREATE EXTENSION pgstattuple;. Once enabled, you can use functions like pgstattuple(relation) to get details about the physical memory used by a table, including the number of pages, live tuples, dead tuples, and more. The function pgstattuple_approx(relation) provides a faster estimate of these metrics. You can also get index statistics using pgstatindex(index). Analyzing this low-level data can help identify bloated tables that need vacuuming, find tables with high dead tuple rates that could benefit from being rewritten, and optimize your database\u0026rsquo;s physical storage usage.\nThe output of pgstattuple provides actionable insights for monitoring, maintenance, and performance tuning, as discussed in the following sections.\nDetecting and managing table bloat Identifying and managing bloat is one of the most useful applications of pgstattuple for PostgreSQL tables. Bloat arises when UPDATE and DELETE operations leave behind unused space that is not automatically reclaimed.\nPostgreSQL maintains data consistency through a Multiversion Concurrency Control (MVCC) model, where each SQL statement sees a snapshot of data from a previous point in time, regardless of the current state of the underlying data. This prevents statements from viewing inconsistent data caused by concurrent transactions updating the same row, providing transaction isolation for each database session. Unlike traditional locking methods, MVCC minimizes lock contention, allowing for reasonable multi-user performance.\nWhen deleting a row in MVCC systems like PostgreSQL, that row is not immediately removed from data pages. Instead, it is marked as deleted or expired for the current transaction but remains visible to transactions viewing an older snapshot, avoiding conflicts. When transactions complete, these dead or expired tuples are expected to be vacuumed and the space reclaimed. In PostgreSQL, an UPDATE operation is equivalent to a combination of DELETE and INSERT. When a row is updated, PostgreSQL marks the old version as expired (like a DELETE) but keeps it visible to older transaction snapshots. It then inserts a new version of the row with updated values (like an INSERT). Over time, expired row versions accumulate until the VACUUM process removes them, reclaiming the space. This approach enables PostgreSQL\u0026rsquo;s MVCC model, providing snapshot isolation without explicit locking during updates.\nPostgreSQL\u0026rsquo;s Autovacuum is an automated maintenance process that helps reclaim memory occupied by dead tuples and updates statistics used by the query planner. The autovacuum process triggers when the maximum age (in number of transactions) crosses autovacuum_freeze_max_age, or when a threshold is reached: autovacuum_vacuum_threshold + autovacuum_vacuum_scale_factor * number of tuples. In this formula, autovacuum_vacuum_threshold represents the minimum number of updated or deleted tuples required to trigger cleanup, while autovacuum_vacuum_scale_factor is a fraction of the table size added to the threshold calculation to determine when maintenance should occur. If autovacuum fails to clean up dead tuples for certain reasons, you may need to handle severely bloated tables manually.\nDead tuples are stored alongside live tuples in data pages. Bloat can also be due to free space within pages, for example, after autovacuum has cleaned up dead tuples. During query execution, PostgreSQL scans more pages filled with dead tuples, causing increased I/O and slower queries. Severely bloated tables cause database workloads to consume unnecessary read I/O, impacting application performance. Cleaning up bloat may be necessary if autovacuum fails.\nBefore we dive into analyzing table bloat with pgstattuple, ensure you have everything set up to follow along. You will need access to an Amazon RDS or Aurora PostgreSQL instance, as well as a client with psql installed and properly configured to connect to your database. Make sure you have the necessary permissions to create tables and install extensions in your PostgreSQL environment. For this demonstration, we will use the pgbench_accounts table. If you don\u0026rsquo;t have this table yet, you can easily create it using the pgbench utility. Run the command pgbench -i -s 10 to initialize a pgbench schema with a scale factor of 10, which will create the pgbench_accounts table along with other necessary tables. This will provide us with sample data to work with during the analysis. Additionally, you should install the pgstattuple extension on your database instance. If you haven\u0026rsquo;t installed it yet, you can do so by running CREATE EXTENSION pgstattuple; as a user with sufficient privileges. With these prerequisites, you will be ready to explore table bloat analysis using real data in a controlled environment.\nWhile pgstattuple provides comprehensive analysis of table bloat, it can be resource-intensive. We recommend first using lighter bloat estimation queries noted here. If more detailed analysis is needed, here is how to use pgstattuple. The following example illustrates how to use pgstattuple to analyze bloat information in a table.\nCreate the table pgbench_accounts_test with 10,000 records: In this example, the pgstattuple query returns a dead tuple count of 0 and a table size of 1672kB: To demonstrate pgstattuple usage, we disable autovacuum (not recommended in production) and update 2,500 records: Now, the pgstattuple data for this table shows 2,500 old version tuples moved to dead tuples. bloat_percentage in PostgreSQL refers to the ratio of space that can be reclaimed in a table or index relative to its total size. It can be calculated using data from pgstattuple as follows: A bloat_percentage value exceeding 30%–40% generally indicates a bloat issue that needs to be addressed. To clean up bloat, use the VACUUM command: Let\u0026rsquo;s check the pgstattuple data after the VACUUM operation: The VACUUM operation resets dead_tuple_count to 0. Free space remains attached to the table and is available for INSERT or UPDATE operations within the same table. This causes table_len (table length) to remain unchanged even after performing the VACUUM operation.\nTo reclaim disk space occupied by bloat, there are two options:\nVACUUM FULL – VACUUM FULL can reclaim more disk space but runs much slower. It requires an ACCESS EXCLUSIVE lock on the table it is processing, and therefore cannot be performed in parallel with other uses of the table. While VACUUM FULL operations are generally not recommended in production environments, they may be acceptable during scheduled maintenance windows where downtime is planned and approved.\npg_repack – pg_repack is a PostgreSQL extension that effectively removes bloat from tables and indexes while maintaining online availability. Unlike CLUSTER and VACUUM FULL, it minimizes exclusive lock times during processing, delivering performance comparable to CLUSTER. Although pg_repack allows for online table and index reorganization with minimal application downtime, it is important to consider its limitations. The extension still requires short exclusive locks during operation and may struggle to complete on high-transaction tables, potentially impacting database performance. For heavily used tables where full repacking is difficult, consider the alternative of index-only repacking. Best practices include thorough testing in non-production environments, scheduling during low traffic periods, and having a monitoring and rollback plan in place. Despite the benefits, users should be aware of potential risks and plan accordingly when implementing pg_repack in their PostgreSQL environments.\nVACUUM FULL operation reduces table_len:\nThe VACUUM FULL operation reclaims wasted space to disk and reduces table_len. The following query identifies bloat in the 10 largest tables in your database using pgstattuple. pgstattuple performs a full table scan and can consume more instance resources like CPU and I/O. This makes pgstattuple operations slower for large tables. Alternatively, the function pgstattuple_approx(relation) provides a faster estimate of these metrics. While less resource-intensive than pgstattuple, it can still be challenging for very large tables or busy systems. Consider running it during off-peak hours or on a replica if available.\nAutomating manual vacuum Regularly monitoring for bloat allows you to proactively identify maintenance needs before performance is impacted. Bloat metrics can also help tune autovacuum settings to clean up space more aggressively if needed. Once you identify the top 10 most bloated tables, you can automate the VACUUM operation using the pg_cron extension. pg_cron is a cron-based job scheduler for PostgreSQL that runs inside the database as an extension. It uses syntax similar to standard cron but allows you to schedule PostgreSQL commands directly from the database. The following code snippet is an example of using pg_cron\u0026rsquo;s cron.schedule function to set up a job that runs VACUUM on a specific table at 23:00 (GMT) daily: Diagnosing and resolving index bloat Just like tables, indexes in PostgreSQL can also get bloated, wasting space and impacting performance. pgstattuple allows detection of index bloat using pgstatindex.\nThe following query displays the index identifier, total index size in bytes, and average leaf density:\nAverage leaf density is the percentage of useful data in index leaf pages. Significantly bloated indexes can be rebuilt using the REINDEX command or pg_repack to remove dead space and restore optimal performance. It is recommended to periodically check for bloat on busy indexes with high churn rates.\nAssessing index fragmentation Another valuable use of pgstattuple is identifying index fragmentation issues. Fragmentation occurs when index pages become scattered due to deletes, updates, and page splits. Heavily fragmented indexes have many dead tuples occupying space inefficiently. We can check the fragmentation level using leaf_fragmentation: If leaf_fragmentation is high, the index is likely fragmented and a REINDEX should be considered. Rebuilding will remove fragmentation and associated performance overheads.\nBest practices when using pgstattuple Consider the following best practices when using pgstattuple for PostgreSQL monitoring and maintenance:\nTo estimate bloat in PostgreSQL tables, use the check_postgres query mentioned on the PostgreSQL wiki. Use the pgstattuple extension to analyze the physical storage of database tables, providing detailed statistics on space usage in the database, including space wasted due to bloat. Rebuild significantly bloated tables and indexes to reclaim dead space. Monitor high dead_tuple_percent to identify fragmentation issues. Focus maintenance on tables and indexes critical to workload performance. Avoid running pgstattuple on highly active tables to prevent interference. Use pgstattuple metrics to fine-tune autovacuum settings. Combine pgstattuple with query analysis and logs for a comprehensive view of the database. Conclusion The pgstattuple extension acts as a powerful tool for discovering key diagnostic metrics in PostgreSQL databases, revealing detailed storage statistics that help teams identify and resolve performance-impacting issues like bloat and index fragmentation. Working seamlessly with Aurora and RDS PostgreSQL, this extension provides necessary visibility into storage patterns and maintenance requirements.\nFollowing pgstattuple best practices is key to maintaining efficient, high-performance PostgreSQL databases, and organizations can further enhance their database management through AWS support options – AWS Enterprise Support, Enterprise On-Ramp, and Business Support customers can leverage AWS Countdown Premium engagements for optimization guidance, allowing teams to confidently implement best practices and maintain optimal performance while focusing on their core business goals.\nAuthor Information Vivek Singh Vivek Singh is a Principal Database Specialist, Technical Account Manager at AWS, focusing on Amazon RDS for PostgreSQL and Amazon Aurora PostgreSQL engines. He works with enterprise customers, providing technical support on PostgreSQL operational performance and sharing database best practices. He has over 17 years of experience in open-source database solutions and enjoys working with customers to help design, deploy, and optimize relational database workloads on AWS.\nKiran Singh Kiran Singh is a Senior Partner Solutions Architect and an expert on Amazon RDS and Amazon Aurora at AWS, focusing on relational databases. She helps customers and partners build highly optimized, scalable, and secure solutions; modernize their architectures; and migrate their database workloads to AWS.\nSagar Patel Sagar Patel is a Principal Database Specialty Architect for the Professional Services team at Amazon Web Services. He works as a database migration expert, providing technical guidance and support to Amazon customers migrating their on-premises databases to AWS.\n"},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.4-databaseandstorage/5.4.1-create-rds/","title":"Create RDS","tags":[],"description":"","content":" Open the Amazon Aurora and RDS\nIn left navbar, choose Databases, then click Create database In create console, choose Full configuration\nThen choose database type is MySQL\nChoose Templates is Production Select Availability and durability is Multi-AZ DB cluster deployment (3 instances)\nFill DB instance identifier\nChoose Self managed\nSpacific Master password In Instance configuration, choose db.m5d.large\nIn Storage, Allocated storage fill in 100 and Provisioned IOPS is 1000\nIn Connectivity, choose Don\u0026rsquo;t connect to an EC2, then select VPC Choose DB subnet group and then in VPC security group (firewall) select Choose existing\nSelect rds-sg\nThen click Create database "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.3-vpc/5.3.1-create-route-table/","title":"Create Route table and Internet Gateway","tags":[],"description":"","content":" A Route Table is a set of rules (routes) used by a router to determine the best path for data packets to reach their destination.\nOpen the Amazon VPC console Choose Route tables, then click Create route table In the Create route table console: Specify name of Route table Choose VPC created Then click Create route table In Route table console, click Route table created Choose Route in navbar -\u0026gt; click Edit routes Edit routes console -\u0026gt; choose Target local -\u0026gt; Save changes Choose Internet Gateway in left navbar -\u0026gt; click Create internet gateway In Create Internet Gateway Specify name of Internet Gateway Click Create internet gateway Back to Route table -\u0026gt; Create route table like step 3 Click this Route table -\u0026gt; Edit routes In Target choose Internet Gateway created Then click **Save changes "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Lai Hoang Minh Phuc\nPhone Number: 0335828579\nEmail: phuclhmse184914@fpt.edu.vn University: FPT HCM University\nMajor: AI\nClass: AWS\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 14/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"ApexEV — Workshop Introduction (Short) ApexEV is an enterprise-grade EV garage management platform that digitizes workshop operations, improves customer experience, and helps technicians work faster and safer.\nWhy this workshop:\nSolve common garage problems: manual processes, poor transparency, weak customer care, and data risk. Build a secure, scalable and cost-efficient cloud backend from day one using AWS best practices. Core architecture (summary):\nFrontend: React app hosted on AWS Amplify (CI/CD + CloudFront). Backend: Spring Boot services in ECS (Fargate) — serverless containers. Database: Amazon RDS (private subnets, automated backups, KMS encryption). Storage: Amazon S3 for media (use presigned URLs for direct uploads). API + Async: API Gateway as HTTPS ingress; SNS → Lambda → SES for email; Lambda → Bedrock for AI/chat. Network \u0026amp; Security: VPC (public/private), Security Groups, VPC Endpoints, WAF and least-privilege IAM. Key benefits:\nSecurity-first: backend and DB remain private; edge services terminate TLS and enforce WAF/rate limits. Cost-aware: Fargate + Lambda (pay-per-use), lifecycle rules and autoscaling reduce costs. Modern \u0026amp; modular: frontend/backend separation, event-driven async flows for resilience and scale. Workshop goals:\nProvision network and secure services, deploy frontend + backend, connect RDS and S3, and integrate email and AI pipelines. Each module includes steps, recommended settings and cleanup instructions. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Integrate into the AWS work culture and get to know new colleagues during the OJT process at First Cloud Journey (FCJ). Understand foundational knowledge of AWS Core Services. Practice operations on the AWS Management Console. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Mon - Preliminary research on AWS - Learn about the rules and regulations at the internship unit 09/08/2025 09/08/2025 Tue - Learn AWS through Module 1 + AWS Infrastructure + AWS Management Tools + Cost Optimization Practice: Create an AWS account 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/watch?v=HxYZAK1coOI https://www.youtube.com/watch?v=IK59Zdd1poE https://www.youtube.com/watch?v=HSzrWGqo3ME https://www.youtube.com/watch?v=pjr5a-HYAjI https://www.youtube.com/watch?v=2PQYqH_HkXw https://www.youtube.com/watch?v=IY61YlmXQe8 https://www.youtube.com/watch?v=Hku7exDBURo Wed - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Interact with AWS Console + Set up MFA and IAM + Install \u0026amp; configure AWS CLI 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ Thu - Learn about EC2: + Instance Types + Storage: EBS (Elastic Block Store) and Instance Store (Ephemeral Storage) + Network \u0026amp; Security: Security Groups, Key Pairs, and Public IP vs Private IP vs Elastic IP + IAM Role + Learn about SSH 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ Fri - Practice: + Launch an EC2 instance + Connect to that EC2 instance via SSH 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Mastered the rules and working regulations at the FCJ internship unit.\nUnderstood the overview of AWS global infrastructure architecture, management tools, and cost optimization principles (Module 1).\nSuccessfully created and activated an AWS Free Tier account.\nFamiliarized with the AWS Management Console, performed account security setup:\nConfigured MFA (Multi-Factor Authentication). Created and managed basic IAM Users/Groups. Successfully installed and configured AWS CLI on a personal computer.\nMastered foundational knowledge of the Amazon EC2 service, including:\nInstance Types classification. Storage options: Distinguish between EBS (Elastic Block Store) and Instance Store. Network \u0026amp; Security mechanisms: Security Groups, Key Pairs, distinguishing Public/Private/Elastic IP. The role of IAM Role in access delegation. Successfully practiced with EC2 service:\nLaunched a complete EC2 Instance. Used SSH protocol to connect and interact with the Instance from the workstation. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Deeply understand AWS networking architecture: VPC, security features, and Multi-VPC models. Master advanced networking concepts: VPN, DirectConnect, and LoadBalancer. Practice building a complete network system: Subnet, Route Table, Internet Gateway, Security Group, and Network ACLs. Connect and establish teamwork with final project team members. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn about AWS Virtual Private Cloud (VPC) - Learn about VPC Security and Multi-VPC features 15/09/2025 15/09/2025 https://www.youtube.com/watch?v=O9Ac_vGHquM https://www.youtube.com/watch?v=BPuD1l2hEQ4\u0026t Tue - Learn about VPN - DirectConnect - LoadBalancer - ExtraResources 16/09/2025 16/09/2025 https://www.youtube.com/watch?v=CXU8D3kyxIc\u0026t Wed - Practice: + Get familiar with VPC + Build Subnets + Learn about Route Table and Internet Gateway 17/09/2025 17/09/2025 https://www.youtube.com/watch?v=dHoYmQR7FYs https://www.youtube.com/watch?v=XBJgHS3XQjk Thu - Learn and practice Security Groups - Learn and practice Network ACLs - Learn and practice VPC Resource Map 18/09/2025 18/09/2025 https://www.youtube.com/watch?v=B1qxOQLmavQ https://www.youtube.com/watch?v=GVDsDu9dOFY\u0026t https://www.youtube.com/watch?v=fZa_kQ69stI Fri - Review what was learned during the week - Expand knowledge on VPC - Meeting to meet and get to know final project team members 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Clearly understand and distinguish core Amazon VPC components:\nCIDR block Subnet (Public vs Private) Route Table \u0026amp; Internet Gateway (IGW) Grasp networking connectivity and load distribution solutions:\nVPN \u0026amp; DirectConnect (Hybrid Cloud connectivity) Load Balancer (Application load balancing) Setup and manage network security layers:\nSecurity Group: Configure Instance-level firewall (Stateful). Network ACLs: Configure Subnet-level firewall (Stateless). Proficiently use the VPC Resource Map tool to visualize and control resource traffic flow within the network.\nCompleted the first meeting with the project team.\n"},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Master EC2 compute services: Lifecycle, Storage (EBS/Instance Store), and Auto Scaling. Implement advanced connectivity using AWS Transit Gateway. Deploy scalable storage and content delivery solutions (S3 \u0026amp; CloudFront). Explore foundational AI/ML concepts (NLP, Sentiment Analysis) for the final project. Collaborate with the team to brainstorm and finalize the final project idea. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Create EC2 Instances in Subnets - Practice creating Internet Gateway - Learn about Transit Gateway Route Tables - Install Transit Gateway - Connect EC2 Instance to Endpoint 22/09/2025 22/09/2025 https://www.youtube.com/@AWSStudyGroup/ https://cloudjourney.awsstudygroup.com/ Tue - Deep dive into EC2 + AMI/ Backup/ Key pair + Elastic Block Store (EBS) + Instance Store + User data \u0026amp; meta data + EC2 Auto Scaling\n23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/@AWSStudyGroup/ Wed - Deploy Infrastructure - Create Backup plan - Conduct Recovery Testing - Clean up resources - Create S3 Bucket 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/@AWSStudyGroup/ Thu - Create EC2 for Storage Gateway - Practice creating a simple static website - Configure public access block and public objects - Learn about AWS CloudFront and practice configuring CloudFront 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ https://www.youtube.com/@AWSStudyGroup/ Fri - Learn Supervised ML \u0026amp; Sentiment Analysis - Natural Language preprocessing - Visualizing tweets and Logistic Regression models - Team meeting to brainstorm ideas for the final project 26/09/2025 26/09/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Week 3 Achievements: Successfully managed and optimized EC2 Instances:\nConfigured AMIs, Key Pairs, and User Data/Meta Data. Differentiated between Elastic Block Store (EBS) and Instance Store. Implemented EC2 Auto Scaling for high availability. Built complex network topologies using AWS Transit Gateway to interconnect VPCs.\nExecuted Disaster Recovery and Storage operations:\nCreated Backup plans and successfully performed recovery testing. Deployed a Static Website using S3 Buckets. Configured Public Access settings and Object permissions. Optimized content delivery performance:\nIntegrated AWS CloudFront (CDN) with S3 to reduce latency and improve access speeds. Gained initial exposure to Machine Learning (NLP):\nUnderstood Supervised ML \u0026amp; Sentiment Analysis concepts. Practiced Natural Language preprocessing and Logistic Regression visualization. Defined the initial concept for the final project. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Master Hybrid Cloud workflows: Importing/Exporting Virtual Machines (VMs) and integrating On-premises storage. Deepen knowledge of AWS Storage (EFS, FSx, Storage Gateway) and Compute (Autoscaling, Lightsail). Build the mathematical foundation for Natural Language Processing (NLP): Vector Spaces, Probability, and Linear Algebra. Finalize the semester project concept and setup the documentation framework using Hugo. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn about EC2 Autoscaling - EFS/FSx - Lightsail - Supplement knowledge on Basic Storage \u0026amp; Compute services on AWS - Learn about Probability and Bayes’ Rule 29/09/2025 29/09/2025 https://www.youtube.com/@AWSStudyGroup/ https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Tue - Expand knowledge on Database and Security services on AWS - Learn VMWare Workstation - Practice exporting Virtual Machine from On-premises - Practice uploading Virtual Machine to AWS 30/09/2025 30/09/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Learn Linear algebra in Python with Numpy - Learn Euclidean Distance \u0026amp; Cosine Similarity - Learn Manipulating Words in Vector Spaces - Practice code labs on Vector Space Models 01/10/2025 01/10/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Thu - Import Virtual Machine into AWS - Deploy Instance from AMI - Configure S3 bucket ACLs - Export VM from Instance - Clean up AWS Cloud resources - Create Storage Gateway - Mount File shares on On-premises machine 02/10/2025 02/10/2025 https://www.youtube.com/@AWSStudyGroup/ Fri - Learn about Hugo Themes - Learn how to write the Final Workshop Report - Summary meeting and finalize the ultimate idea for the final report with the group 03/10/2025 03/10/2025 https://www.youtube.com/@AWSStudyGroup/ Week 4 Achievements: Successfully implemented Hybrid Cloud strategies:\nExported VMs from local VMWare Workstation and uploaded them to AWS. Imported VMs to create AMIs and launched functional Instances. Exported AWS Instances back to local environments. Configured Advanced Storage Solutions:\nDeployed AWS Storage Gateway. Successfully mounted AWS file shares directly onto on-premises machines. Managed S3 Access Control Lists (ACLs) for granular security. Applied Mathematical Foundations for NLP (Machine Learning):\nUtilized Python (Numpy) for Linear Algebra operations. Calculated Euclidean Distance \u0026amp; Cosine Similarity for text analysis. Understood and applied Bayes’ Rule and Vector Space Models. Project \u0026amp; Documentation Readiness:\nSelected and configured the Hugo Theme for the final report. Reached a consensus on the final project idea with the team. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master advanced NLP algorithms: Vector transformation, K-nearest neighbors (KNN), and Hash tables. Deep dive into the AWS Storage ecosystem: S3 advanced features, Glacier, Snow Family, and Backup strategies. Implement Security compliance and Automation: Security Hub, IAM Roles, and AWS Lambda. Finalize the detailed execution plan for the final semester project. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn about Transforming word vectors - Learn K-nearest neighbors - Learn about Hash tables and hash functions - Complete Word Translation code lab 06/10/2025 06/10/2025 https://www.coursera.org/learn/classification-vector-spaces-in-nlp/ Tue - Learn AWS Storage Services - Learn Amazon Simple Storage Service (S3) - Access Points - Storage Classes - Learn S3 Static Website \u0026amp; CORS - Control Access - Object Key \u0026amp; Performance - Glacier - Learn Snow Family - Storage Gateway - Backup 07/10/2025 07/10/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Complete Lab Module 05 + Enable Security Hub + Score for each set of criteria + Clean up resources + Create VPC + Create Security Group and EC2 Instance + Tag Instance + Create Role for Lambda + Create Lambda Function + Check results and clean up 08/10/2025 08/10/2025 https://www.youtube.com/@AWSStudyGroup/ Thu - Translate Blog - Meeting to build the final project plan - Complete Lab Module 05-28 - Complete Lab Module 05-29 - Complete Lab Module 05-30 09/10/2025 09/10/2025 https://www.youtube.com/@AWSStudyGroup/ Fri - Complete Lab Module 05-31 - Complete Lab Module 05-32 - Complete Lab Module 05-33 - Team meeting to summarize the final project construction plan - Review knowledge from the week 10/10/2025 10/10/2025 https://www.youtube.com/@AWSStudyGroup/ Week 5 Achievements: Mastered Vector Space arithmetic in NLP:\nImplemented K-nearest neighbors (KNN) and Hash tables. Successfully coded a Word Translation model using vector transformation. Configured Advanced AWS Storage Architectures:\nDeployed S3 Static Websites with CORS configurations. Managed S3 Storage Classes and Access Control. Understood offline data migration using AWS Snow Family. Implemented Cloud Security and Automation:\nEnabled and scored compliance using AWS Security Hub. Created Serverless automation workflows using AWS Lambda and IAM Roles. Managed EC2 resources via tagging strategies. Project Progress:\nCompleted extensive lab series (Module 05). Finalized and agreed upon the detailed roadmap for the final project. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Master AWS Security fundamentals: Shared Responsibility Model, Identity Center, and Key Management. Develop skills in designing and visualizing Cloud Architecture using professional standards (Draw.io). Deep dive into NLP Attention Models: Seq2seq, Neural Machine Translation (NMT), and Evaluation metrics. Collaborate with the team to draft and refine the High-Level Architecture for the final project. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn how to draw AWS architecture diagrams - Get familiar and practice drafting with Draw.io - Complete Lab Module 05-44 - Complete Lab Module 05-48 13/10/2025 13/10/2025 https://www.youtube.com/@AWSStudyGroup/ Tue - Learn Shared Responsibility Model - Learn Amazon Identity \u0026amp; Access Management (IAM) - Learn AWS Cognito - Learn AWS Organizations - Learn AWS Identity Center - Learn Amazon Key Management Service (KMS) - Learn AWS Security Hub - Research, practice, and supplement daily learning 14/10/2025 14/10/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Meeting to draft the overview architecture diagram of services to be used - Learn Seq2seq model - Learn Queries, Keys, Values, and Attention - Learn Seq2seq Model with Attention - Learn NMT Model, Machine Translation - Learn BLEU Score \u0026amp; ROUGE-N Score - Learn Beam Search \u0026amp; Minimum Bayes Risk 15/10/2025 15/10/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Thu - Learn the structure of AWS architecture diagrams - Practice drawing and editing diagrams 16/10/2025 16/10/2025 https://www.youtube.com/@AWSStudyGroup/ Fri - Review knowledge from the week - Continue editing and asking for feedback from mentors/seniors regarding the group architecture diagram - Group meeting to discuss the AWS architecture diagram 17/10/2025 17/10/2025 https://www.youtube.com/@AWSStudyGroup/ Week 6 Achievements: Solidified Cloud Security Knowledge:\nUnderstood the Shared Responsibility Model. Configured IAM and AWS Identity Center for user management. Explored AWS Cognito for app authentication and KMS for encryption. Professional Architecture Design:\nProficient in using Draw.io with AWS Icons sets. Drafted the initial High-Level Architecture for the final project. Received and implemented feedback on the diagram structure. Mastered Advanced NLP Concepts (Attention Models):\nUnderstood the mechanics of Seq2seq and the Attention mechanism (Queries, Keys, Values). Learned to evaluate Machine Translation models using BLEU and ROUGE-N scores. Grasped sampling techniques like Beam Search. Team Collaboration:\nUnified the team\u0026rsquo;s vision on the system architecture through structured meetings. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Master AWS Database services: RDS, Aurora, Redshift, and ElastiCache. Develop practical AI skills: Research and implement an AI Chatbot. Finalize the High-Level Architecture Diagram based on mentor feedback. Kick-start the Final Project implementation: Frontend and Backend design. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Complete Lab Module 06 - Continue editing the Architecture Diagram 20/10/2025 20/10/2025 https://www.youtube.com/@AWSStudyGroup/ Tue - Learn Database Concepts - Learn about Amazon RDS \u0026amp; Amazon Aurora - Learn about Redshift - Elasticache - Learn how to create an AI chatbot 21/10/2025 21/10/2025 https://www.youtube.com/@AWSStudyGroup/ Wed - Practice creating an AI chatbot - Research and supplement AI knowledge regarding chatbots - Revise the diagram based on reviews from mentors in the group 22/10/2025 22/10/2025 https://www.youtube.com/ Thu - Commence the Final Project - Design Front-End \u0026amp; Back-end 23/10/2025 23/10/2025 Fri - Review knowledge from the week - Continue editing and asking for feedback from group members regarding the architecture diagram - Test AI and design Front-end and Back-end parts 24/10/2025 24/10/2025 Week 7 Achievements: Deep understanding of AWS Database Ecosystem:\nDistinguished between Relational (RDS, Aurora) and Data Warehousing (Redshift) services. Understood In-memory caching strategies using Amazon ElastiCache. AI \u0026amp; Machine Learning Implementation:\nSuccessfully researched and prototyped a functional AI Chatbot. Integrated AI logic into the project scope. Architecture \u0026amp; Design Finalization:\nRefined and finalized the System Architecture Diagram after multiple rounds of feedback/review. Project Execution:\nOfficially launched the coding phase for the Final Project. Completed initial designs for both Frontend (UI/UX) and Backend structure. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Finalize the System Architecture and integrate the AI module into the main project. Complete the core development phase: Basic Frontend, Backend, and Project Proposal. Comprehensive review of all AWS modules and supplementary knowledge covered to date. Successfully complete the Midterm Exam. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Revise the architecture diagram and swap out certain services - Finalize AI and proceed to integrate it into the project 27/10/2025 27/10/2025 Tue - Write Project Proposal - Team collaboration meeting - Complete the basic Front-end and Back-end 28/10/2025 28/10/2025 Wed - Review everything learned so far - Review additional supplementary knowledge 29/10/2025 29/10/2025 https://www.youtube.com/@AWSStudyGroup/ https://cloudjourney.awsstudygroup.com/ Thu - Review everything learned so far - Review additional supplementary knowledge 30/10/2025 30/10/2025 https://www.youtube.com/@AWSStudyGroup/ https://cloudjourney.awsstudygroup.com/ Fri - Take Midterm Exam - Meeting to adjust a few project functions 31/10/2025 31/10/2025 Week 8 Achievements: System Integration \u0026amp; Optimization:\nSuccessfully integrated the AI Chatbot/Model into the main application. Revised the AWS Architecture Diagram to optimize service selection based on practical implementation needs. Project Development Milestones:\nCompleted the fundamental structure for both Front-end and Back-end. Finalized and documented the Project Proposal. Knowledge Consolidation \u0026amp; Assessment:\nConducted a comprehensive review of all previous weeks\u0026rsquo; modules (Compute, Storage, Networking, Database, Security). Completed the Midterm Exam. Agile Adaptation:\nAdjusted project functionality based on team meetings and testing results. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Deep dive into modern NLP architectures: Transformers, Attention Mechanisms, and Decoders. Explore Transfer Learning and Large Language Models (LLMs) like BERT, GPT, and T5. Implement Generative AI solutions using AWS Bedrock. Optimize the Final Project architecture and refine the Chatbot\u0026rsquo;s performance/UI. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn about Transformers vs RNNs - Learn about Scaled and Dot-Product Attention - Learn about Masked Self Attention - Learn about Transformer Decoder 03/11/2025 03/11/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Tue - Complete practice code lab on Transformer Summarizer - Optimize Chatbot and Interface 04/11/2025 04/11/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Wed - Meeting to change/optimize AWS services for the project - Learn Transfer Learning in NLP - Learn about Large Language Models: ELMo, GPT, BERT, T5 05/11/2025 05/11/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Thu - Learn about AWS Bedrock - Experiment with AI Chatbot using AWS Bedrock 06/11/2025 06/11/2025 Fri - Search and generate data for the Chatbot created with AWS Bedrock - Test Chatbot and fix processing errors 07/11/2025 07/11/2025 Week 9 Achievements: Mastered Advanced NLP Architectures:\nDifferentiated between RNNs and Transformers. Understood core mechanisms: Scaled Dot-Product Attention and Masked Self-Attention. Implemented a Transformer Summarizer via code labs. Understanding Large Language Models (LLMs):\nGained insight into Transfer Learning strategies. Explored the evolution of major models: ELMo, GPT, BERT, and T5. Applied Generative AI with AWS:\nSuccessfully integrated AWS Bedrock to upgrade the AI Chatbot capabilities. Curated datasets for fine-tuning/context and resolved integration bugs. Project Optimization:\nRefined the User Interface (UI) for better user experience. Re-evaluated and optimized the AWS service stack for the final product. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/","title":"Worklog","tags":[],"description":"","content":"The internship roadmap was divided into three main phases:\nFoundational Knowledge: Mastering AWS core services (Networking, Compute, Storage, Security) and Math for ML. Advanced Research: Deep diving into NLP, Transformers, and Generative AI (AWS Bedrock). Project Implementation: System architecture design, Full-stack development, and Serverless deployment. Below is the weekly breakdown of tasks and achievements:\nWeek 1: Introduction to Cloud Computing \u0026amp; AWS Account Setup\nWeek 2: Deep Dive into AWS Networking: VPC \u0026amp; Security\nWeek 3: Compute, Connectivity (Transit Gateway) \u0026amp; Intro to NLP\nWeek 4: Hybrid Cloud Solutions, Storage Gateway \u0026amp; Math for NLP\nWeek 5: Advanced Storage, Security Automation \u0026amp; NLP Algorithms\nWeek 6: Cloud Architecture Design, Identity Security \u0026amp; Attention Models\nWeek 7: Databases, AI Chatbot Prototyping \u0026amp; Project Kick-off\nWeek 8: System Integration, Midterm Exam \u0026amp; Project Proposal\nWeek 9: Transformers, LLMs \u0026amp; Generative AI with AWS Bedrock\nWeek 10: BERT Fine-tuning, Serverless Optimization \u0026amp; UI Completion\nWeek 11: Documentation, Final Deployment \u0026amp; Architectural Synchronization\nWeek 12: Final Review, Bug Fixing \u0026amp; Product Demo\n"},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Consolidated Report: “AWS AI/ML and Generative AI Workshop” Event Objectives Gain an overview of AWS AI/ML services (Amazon SageMaker) and the Generative AI platform (Amazon Bedrock). Deeply understand the AIDLC (AI-Driven Development) methodology to accelerate software development. Master the Kiro IDE tool and new concepts like Spec-Driven Development (SDD) and Agent Hooks. Learn effective Prompt Engineering and Context Management for working with AI. Key Highlights AWS AI/ML Services \u0026amp; Bedrock The program presented a comprehensive landscape of AWS AI capabilities:\nAmazon SageMaker: An end-to-end ML platform covering data preparation, training, fine-tuning, and integrated MLOps. Amazon Bedrock: The flagship service for Generative AI, enabling the selection of Foundation Models (Claude, Llama, Titan). Advanced Features: Discussions on RAG (Retrieval-Augmented Generation) architecture, Knowledge Base integration, Bedrock Agents for multi-step workflows, and Guardrails for content filtering. AIDLC Methodology (AI-Driven Development) This was the core of the workflow transformation, reducing development time from 2 weeks to 1.5 days:\nHuman-Centric Philosophy: Humans are responsible for validation, decisioning, and oversight. AI is never allowed to make decisions autonomously. Three-Phase Process: Inception (Defining Requirements/User Stories), Construction (Implementing Units), and Operation (Deploy/CICD). Mob Development: Proposed a team model where multiple roles (BA, Engineer, SA, QA) work together on a single machine to validate outputs immediately. Kiro IDE \u0026amp; Spec-Driven Development Exploring the next-gen AI-native IDE (similar to Coder/VS Code):\nSpec-Driven Development (SDD): Starting with specification documents instead of coding. Kiro automatically generates requirement, design, and task list files. Agent Hooks: A feature for event-driven automation using natural language (e.g., automatically running tests upon saving a file). Advanced Context Management: Automatically executing AIDLC workflows via a steering configuration file. Context Management Input Optimization: AI understands natural language context better than raw code. Code should be extracted into project summaries or domain models. Session Control: Create separate sessions for each task (Unit of Work) to avoid context overload and minimize hallucinations. Key Takeaways Mindset for Working with AI Planning is Mandatory: When requesting AI to perform tasks, it is mandatory to ask it to create a Plan first. Iterate on the Plan until it is correct before proceeding. \u0026ldquo;Do, Don\u0026rsquo;t Ban\u0026rdquo; Approach: Instead of telling AI \u0026ldquo;don\u0026rsquo;t do X\u0026rdquo;, give specific instructions to \u0026ldquo;do Y\u0026rdquo;. Negative constraints are generally less effective than affirmative ones. The New Developer Role: Shifting from a \u0026ldquo;coder\u0026rdquo; to a supervisor, responsible for validation and decision-making. Prompting Techniques Define Persona: Always clearly define the AI\u0026rsquo;s role from the beginning. Clear Input/Output: Specify the exact output format (e.g., Markdown file) instead of letting AI store it in temporary memory. Application Adopt AIDLC Process: Start breaking projects down into Inception, Construction, and Operation phases, and practice generating Plans before coding. Switch IDE Tools: Experiment with Kiro IDE or similar AI-assisted tools, utilizing Agent Hooks to automate testing workflows. Optimize Context: Build a habit of writing architectural summaries and domain models to feed context to AI instead of dumping the entire source code. Practice Mob Development: Organize focused team sessions to validate AI-generated results instantly. Event Experience Attending the \u0026ldquo;AWS AI/ML and Generative AI\u0026rdquo; Workshop provided a completely fresh perspective on the future of programming. Memorable experiences included:\nShift in Speed and Mindset I was truly impressed by the AIDLC concept, especially its ability to drastically shorten development time. It is not just a faster tool, but a completely different approach to problem-solving. The \u0026ldquo;Self-Driving Car\u0026rdquo; Analogy The illustration comparing software development to controlling a self-driving car was profound. I realized I don\u0026rsquo;t need to drive every meter, but I must hold the map (Plan) and be ready to brake (Validation) to ensure safety. The Power of Amazon Bedrock The demo on Bedrock Agents and RAG showed immense potential for building intelligent applications capable of reasoning and retrieving enterprise data, going far beyond simple chatbots. Approach to Kiro IDE Learning about Spec-Driven Development (SDD) in Kiro, while noted as somewhat rigid for large projects, opened up a great direction for rapid prototyping and minimizing errors right from the design phase. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.4-databaseandstorage/5.4.2-create-s3/","title":"Create an AWS S3","tags":[],"description":"","content":"In this section you will create S3 bucket to storage photos\nOpen the Amazon S3 Click Create bucket In create console, fill in name of bucket Then leave everything as default like picture Click Create bucket bottom "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.3-vpc/5.3.3-create-security-groups/","title":"Create Security Groups","tags":[],"description":"","content":" Open the Amazon VPC console Choose Security Groups -\u0026gt; click Create security group In Create Security group Spacific name of Security group Choose VPC created Add rule Inbound and Outbound for Security Group In ReGenZet project we have 4 Security groups, which are fargate-sg, rds-sg, alb-sg and endpoint-sg. fargate-sg is security group for AWS ECS Fargate Do again step 1 -\u0026gt; 3 Choose fargate-sg created Inbount Add rule security group of Application Load Balancer (ALB) is alb-sg Outbound Add rule security group of MySQl (rds-sg) and HTTPS Click Create security group rds-sg is security group for AWS RDS Do again step 1 -\u0026gt; 3 Choose rds-sg created Inbound Add rule security group of MySQL like this instruct Outbound is not Add rule Click Create security group alb-sg is security group for AWS Application Load Balancer (ALB) Do again step 1 -\u0026gt; 3 Inbound Add rule HTTPS and HTTP type like this instruct Outbound Add rule security group of AWS ECS Fargate (fargate-sg) Click Create security group endpoint-sg is security group for VPC Endpoints Do again step 1 -\u0026gt; 3 Inbound Add rule security group of AWS ECS Fargate (fargate-sg) Outbound is not Add rule "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.3-vpc/5.3.2-create-subnets/","title":"Create Subnets","tags":[],"description":"","content":"Create Public Subnet Open the Amazon VPC console Choose Subnets -\u0026gt; click Create subnet In Create subnet console: Choose VPC created Fill subnet name Choose AZ Spacific IPv4 subnet Then click Create subnet Create Private Subnet Do again step 1 -\u0026gt; 4 Click this Subnet -\u0026gt; Choose Route table Choose Route table ID is private Click Save "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.3-vpc/5.3.4-create-vpc-endpoints/","title":"Create VPC Endpoints","tags":[],"description":"","content":" Open the Amazon VPC console Choose Endpoints -\u0026gt; click Create endpoints In Create console: Fill name of VPC endpoint Type is AWS Services Search In this project we have 5 VPC Endpoints VPC Enpoint S3 Gateway (apexev-s3-gateway) In search box -\u0026gt; com.amazonaws.ap-southeast-1.s3 Choose type Gateway Choose VPC created Choose Route table private Policy is Full access Click Create endpoint VPC Enpoint ECR API \u0026amp; DKR Interface In search box -\u0026gt; ecr Will see ecr.api and ecr.dkr interface Do this twice and choose a different one each time Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint VPC Enpoint Logs In search box -\u0026gt; com.amazonaws.ap-southeast-1.logs Choose com.amazonaws.ap-southeast-1.logs Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint VPC Enpoint AWS SNS In search box -\u0026gt; com.amazonaws.ap-southeast-1.sns Choose com.amazonaws.ap-southeast-1.sns Choose VPC created Tick two subnet private in two difference AZ Choose endpoint-sg Click Create endpoint "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.2-project-architecture/","title":"Project Architecture","tags":[],"description":"","content":"Architecture Overview This document describes the recommended production-ready architecture for ReGenZet — an EV Garage Management System — and explains the technology choices used in the workshop.\nFrontend: A React single-page application deployed and hosted via AWS Amplify. Amplify provides automated CI/CD, asset hosting and integration with CloudFront for global caching and fast client delivery. Backend: Containerized Spring Boot services packaged as Docker images and deployed to Amazon ECS (Fargate). Fargate removes host management overhead and provides scalable, serverless compute for microservices. Database: Amazon RDS (MySQL/Postgres) hosted in private subnets. RDS provides automated backups, snapshots, Multi-AZ options and encryption at rest (KMS). API Management: Amazon API Gateway exposes a single HTTPS entry point for client traffic, handles request routing, TLS termination and request throttling. Media Storage: Amazon S3 holds vehicle inspection images, videos and other media. Use presigned URLs for secure direct-upload/download to offload traffic from the backend. Asynchronous / Serverless components: Email pipeline: Backend (Spring) publishes an event to SNS, which triggers a Lambda to send email via Amazon SES. AI/Chat pipeline: Frontend → API Gateway → Lambda → Amazon Bedrock (or other managed LLM) for inference and conversational workflows. Network \u0026amp; Security: Deploy resources inside a VPC with well-defined Public and Private subnets. Use Security Groups and NACLs to control traffic. Use VPC Endpoints (Gateway and Interface) for S3 and private service access, keeping traffic inside the AWS network. Why this architecture? Security-first\nThe backend and RDS instances reside in private subnets and never expose database ports to the public internet. API Gateway and load-balanced frontends terminate TLS at the edge, while internal services communicate over private networking. Cost-efficiency\nFargate and Lambda offer a pay-for-what-you-use model. When appropriate, consider Fargate Spot for non-critical workloads and configure autoscaling and lifecycle policies for S3 to reduce costs. Operational simplicity \u0026amp; modern patterns\nClear separation of concerns between frontend and backend, with API Gateway as a single ingress point. Event-driven components (SNS, Lambda) decouple email and AI processing from request-response paths, improving resilience and scalability. Developer productivity\nAWS Amplify simplifies frontend CI/CD and hosting. Container workflows with Docker + ECR and ECS Fargate enable reproducible deployments for backend services. Security and best-practice highlights\nLeast-privilege IAM roles for services and cross-account access where needed. KMS-managed encryption for RDS and S3 objects. WAF and rate-limiting on API Gateway to mitigate application-level attacks. This architecture balances enterprise-grade security with cost-effective serverless patterns and provides a pragmatic path for incremental adoption of advanced features (observability, multi-region DR, Bedrock-powered AI).\n"},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/2-proposal/","title":"Proposal","tags":[],"description":"","content":"APEX-EV Electric Vehicle Service Platform 1. Executive Summary ReGenZ is a comprehensive management platform designed to digitize and optimize maintenance workflows at service centers. The system centrally manages the entire service lifecycle—from request intake and repair processing to customer care—helping to eliminate manual tasks and enhance efficiency. Leveraging the power of the AWS cloud, ReGenZ combines flexible container architecture on Amazon ECS Fargate with the intelligent processing capabilities of Generative AI through Amazon Bedrock. The solution integrates automated development processes (CI/CD) from GitLab, ensuring rapid deployment speeds, high security, and rigorous monitoring, delivering a superior experience for end-users.\n2. Problem Statement What’s the Problem? Current operational processes rely heavily on manual methods, leading to inefficiencies, fragmented data, and a lack of intelligent support tools for automated customer interaction.\nThe Solution The platform employs a modern architecture, starting at the Edge layer with Amazon Route 53 for user routing. The interface (Frontend) is hosted on AWS Amplify Hosting, ensuring fast and stable access. Amazon API Gateway acts as the central hub, intelligently routing requests.\nTo ensure security, critical components such as ECS Fargate and the Amazon RDS database are placed in a Private Subnet, completely isolated from the public internet. Image data is stored on Amazon S3 and accessed securely via S3 Endpoints. Additionally, the software development process is fully automated: source code from GitLab is packaged and pushed to Amazon ECR for deployment to ECS.\nBenefits and Return on Investment Adopting this architecture delivers a significant competitive advantage by integrating Artificial Intelligence (GenAI) via Amazon Bedrock, which helps automate customer care and data analysis. The system ensures high availability and data security thanks to the VPC network separation design (Public/Private subnets).\nThe CI/CD process integrated with GitLab and ECR helps minimize downtime when updating new features, while Amazon CloudWatch provides comprehensive monitoring to detect incidents instantly. The cost model is optimized thanks to the use of Fargate (Serverless container) and Lambda (Pay-per-use), ensuring businesses only pay for the actual resources used. This investment not only resolves current operational challenges but also creates a solid technological foundation for long-term growth, with the expected Return on Investment (ROI) period significantly shortened.\n3. Solution Architecture The ReGenZ management platform utilizes a modern architecture deployed on AWS (Region ap-southeast-2), initiated by user access via Amazon Route 53 at the Edge layer. The User Interface (Frontend) is hosted on AWS Amplify Hosting, which establishes a direct connection to Amazon API Gateway as the central entry point.\nFrom the API Gateway, the data flow is strategically divided into three distinct paths:\nAI Tasks: Requests are routed to AWS Lambda to interact with Amazon Bedrock for generative AI capabilities. Notification Tasks: Asynchronous requests trigger AWS Lambda to handle email communications via Amazon SES. Core Business Logic: Traffic is directed through an Application Load Balancer (ALB) located in the Public Subnet, then forwarded to Amazon ECS Fargate instances secured within a Private Subnet. Data \u0026amp; Security:\nRelational data is persistently stored in Amazon RDS within the Private Subnet. To optimize security and performance, the architecture utilizes VPC Endpoints to keep traffic strictly within the AWS internal network:\nStatic assets and images stored in Amazon S3 are accessed securely via S3 Endpoints. Container images are pulled directly from Amazon ECR via ECR Endpoints. By leveraging these endpoints, the system eliminates the need for a NAT Gateway, thereby reducing costs and minimizing public internet exposure.\nDevOps \u0026amp; Monitoring:\nGitLab is used for source code management and CI/CD, automatically pushing deployments to Amplify (Frontend) and container images to ECR (Backend). AWS Services Used Route 53: DNS service, responsible for routing the domain (Edge layer) to the application. AWS Amplify Hosting: Hosts the web interface (frontend) and can integrate with CDN/WAF. In the diagram, it receives traffic from Route 53. Amazon API Gateway: The main entry point (Gateway), receiving and routing all requests from the frontend/Amplify to processing services. AWS Lambda (Bedrock): Handles AI/Generative AI tasks (prediction/content generation) by communicating with Amazon Bedrock. AWS Lambda (SES): Handles asynchronous tasks, such as processing notifications to send emails via AWS SES. Amazon Bedrock: General AI service (Gen AI), providing foundation models to execute intelligent business operations. AWS SES: Email sending service, performs the sending of notifications, quotes, or processing results from Lambda. VPC: Virtual network environment containing and protecting AWS resources (like ALB, ECS Fargate, RDS). ALB (Application Load Balancer): Load balancer, distributing traffic from API Gateway to application containers running on ECS Fargate. Amazon ECS Fargate: Runs the backend application as containers without server management, handling core business logic. Amazon RDS: Provides a relational database, placed in a Private Subnet to store structured data. Amazon S3: Stores multimedia files like photos or other large data. ECR: Repository for application container images (Docker), used by ECS Fargate for deployment. AWS CloudWatch: Monitoring service, collecting logs and metrics from the entire system to track performance and detect issues. Component Design Request Handling: Amazon Route 53 routes user domain requests to AWS Amplify Hosting, where the frontend interface is hosted. From there, API requests are forwarded to Amazon API Gateway, which acts as the central entry point to receive and route all incoming traffic.\nBusiness Logic Processing:\nCore Logic: All primary business operations are handled by containerized applications running on Amazon ECS Fargate, deployed within a Private Subnet to ensure maximum security. AI \u0026amp; Asynchronous Tasks: Generative AI tasks are processed by AWS Lambda interacting with Amazon Bedrock. Auxiliary tasks, such as email notifications, are handled by separate AWS Lambda functions triggering Amazon SES. Network Infrastructure:\nPublic Subnet: Hosts the Application Load Balancer (ALB) to receive and distribute external traffic. Private Subnet: Dedicated to sensitive resources including ECS Fargate and Amazon RDS, ensuring they are isolated from direct public internet access. VPC Endpoints: The system explicitly utilizes S3 Endpoints and ECR Endpoints. This design allows ECS Fargate to pull container images and access file storage securely within the AWS internal network, without traversing the public internet. Data Storage:\nAmazon RDS: Stores sensitive, structured relational data. Amazon S3: Stores multimedia files and large datasets. Deployment and Monitoring: The deployment pipeline is managed via GitLab, which triggers updates to AWS Amplify (Frontend) and pushes Docker images to Amazon ECR (Backend). Amazon CloudWatch provides comprehensive monitoring of performance logs and metrics across all services, from ECS and Lambda to RDS.\n4. Technical Implementation Implementation Phases\nThe development project for the ReGenZ Smart Electric Vehicle Maintenance Platform — including the integration of an AI virtual assistant and a service management system — is structured into 4 key phases:\nResearch and Architectural Design: Research suitable technologies (React.js, Spring Boot, AWS Bedrock) and design a system architecture combining Containers (ECS) and Serverless (Lambda) on AWS (1 month prior to commencement). Cost Estimation and Feasibility Check: Use the AWS Pricing Calculator to estimate operating costs for core services such as ECS Fargate, RDS, and token costs for Amazon Bedrock, to propose the most financially feasible solution. Architecture Adjustment for Optimization: Refine the architecture, select appropriate configurations for ECS Fargate and RDS, and optimize Lambda runtime (timeouts) to balance AI processing performance and cost. Development, Testing, and Deployment: Develop the React.js application (Frontend) and Spring Boot (Backend), integrate the Bedrock Agent, deploy CI/CD pipelines via GitLab, package Docker images to ECR, and launch operations on ECS. Technical Requirements\nUser Interface (Frontend): Requires practical knowledge of React.js to build scheduling interfaces and chat features with the AI virtual assistant. Utilization of AWS Amplify is necessary to automate the deployment process (Hosting) and connect with Amazon API Gateway to send secure processing requests, ensuring a smooth user experience across all devices.\nCore System (Backend \u0026amp; Infrastructure): Requires in-depth knowledge of Java/Spring Boot to develop maintenance business logic. The application is packaged using Docker, with images stored on Amazon ECR and executed on Amazon ECS Fargate. The team must have a solid understanding of Amazon RDS for relational databases (storing vehicle profiles, maintenance history). Specifically, AWS Lambda (Python) programming skills are required to connect with Amazon Bedrock (for AI/Chatbot processing) and AWS SES (for sending asynchronous email notifications). Finally, system monitoring is implemented using Amazon CloudWatch integration.\n5. Timeline \u0026amp; Milestones Project Timeline\nPhase 1 (Week 1-2): Design and Foundation:\nAnalyze \u0026amp; Design: Define detailed AWS architecture (VPC, Subnets, Security Groups), design Database (RDS Schema), and define APIs (Swagger/OpenAPI). Infrastructure Setup: Configure VPC (Public/Private Subnets) and IAM Roles. CI/CD Setup: Configure Pipeline on GitLab to automatically build Docker Images, push to Amazon ECR, and deploy the Frontend to AWS Amplify. Phase 2 (Week 3-4): Core Service Flow Development:\nCustomer Flow: Develop Registration/Login, Vehicle Profile Management, and Appointment Scheduling (data stored in RDS). Service Advisor Flow: Implement Vehicle Reception, Create Quotations, and Repair Orders. Technician Flow: Develop features to View Task Lists, Update maintenance progress, and upload photos/videos to Amazon S3. Phase 3 (Week 5-6): Administration \u0026amp; Advanced Feature Development:\nAdministration Module: Build Report Dashboard, Spare Parts Management (Inventory), and Personnel Management. AI Integration: Develop AWS Lambda functions to connect with Amazon Bedrock Agent (AI Chatbot) and expose endpoints via API Gateway. Notifications: Develop AWS Lambda functions to trigger AWS SES for sending automatic emails/quotations to customers. Network Security: Configure VPC Endpoints (S3, ECR) to ensure secure, private connectivity between Private Subnet resources and AWS services without using public internet. Phase 4 (Week 7-8): Testing, Optimization, and Operation:\nUser Acceptance Testing (UAT): Conduct internal testing to ensure the flow from Web -\u0026gt; API Gateway -\u0026gt; Lambda/ECS -\u0026gt; DB operates smoothly. Security Optimization: Configure AWS WAF (to block SQL Injection, XSS) and review IAM access rights. Operational Monitoring: Setup Dashboards on Amazon CloudWatch to track logs and metrics of ECS Fargate and Lambda. Official Deployment: Launch the system for production use. 6. Budget Estimation Infrastructure Costs\nAmazon ECS Fargate: ~11.00 USD/month. Application Load Balancer (ALB): ~16.43 USD/month. Amazon Bedrock (AI): ~5.00 USD/month (Calculated by Token count). AWS Lambda: 0.00 USD/month (Free Tier). Amazon RDS \u0026amp; ElastiCache: 0.00 USD/month (Free Tier). S3 Standard: ~0.15 USD/month. AWS Amplify \u0026amp; API Gateway: ~0.50 USD/month. Amazon CloudWatch: 0.00 USD/month (Free Tier). Amazon SES: 0.00 USD/month (Free Tier). Total: ~32.63/month.\n7. Risk Assessment Risk Matrix System Downtime: High impact, Low probability. Security Breach / Data Loss: Very high impact, Low probability. Operational Cost Overrun: Medium impact, Medium probability. Incorrect AI Response (Hallucination): Medium impact, Medium probability. Mitigation Strategies System Stability: Deploy infrastructure across Multi-AZ for RDS and ECS Fargate to ensure high availability. Use Application Load Balancer (ALB) for automatic traffic distribution and health checks. Security: Backend resources (ECS, RDS) are placed in a Private Subnet, isolated from the direct internet access. Access to Amazon S3 and ECR is secured via internal VPC Endpoints, ensuring data never leaves the AWS network. Strict IAM roles are applied to Lambda functions and ECS tasks following the principle of least privilege. Cost Management: Use AWS Budgets to set alerts when costs exceed defined thresholds. Regularly monitor and optimize resources (right-sizing) to avoid waste. AI Quality Control: Limit Amazon Bedrock Agent response scope via strict Prompt Engineering (System Prompts) and only allow information retrieval from moderated Knowledge Bases. Contingency Plans Data Recovery: Enable Automated Backups for Amazon RDS and utilize Point-in-Time Recovery to restore data to any specific moment in case of corruption. Scalability: Configure Auto-scaling for ECS Fargate to automatically expand the number of Tasks when traffic spikes, preventing overload. 8. Expected Outcomes Technical Improvements: Technical Improvements: Successfully build a modern Hybrid Architecture combining Microservices (ECS Fargate) and Serverless (Lambda, Bedrock), ensuring flexible scalability without managing physical servers.\nLong-term Value Enhance customer experience: AI virtual assistant operating 24/7 helps reduce waiting time, increasing appointment conversion rates and car owner satisfaction.\nData assets: Maintenance history and interaction behavior data are centrally stored on RDS/S3, creating a premise for deploying AI Predictive Maintenance models for electric vehicle batteries and motors in the future.\n"},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Master advanced NLP techniques: BERT architecture, Fine-tuning, and Multi-Task Training strategies. Finalize the product interface (UI) and conduct comprehensive system testing. Optimize system architecture by shifting AI integration to a Serverless model. Implement direct Frontend-to-Bedrock integration using AWS Lambda. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Learn about Bidirectional Encoder Representations from Transformers (BERT) - Learn how to fine-tune the BERT model - Learn about Multi-Task Training Strategy - Complete code lab on fine-tuning BERT model based on available data 10/11/2025 10/11/2025 https://www.coursera.org/learn/attention-models-in-nlp/ Tue - Complete User Interface (UI) - Finalize basic functions - Conduct product testing 11/11/2025 11/11/2025 Wed - Meeting to discuss adding certain features - Optimize the system 12/11/2025 12/11/2025 Thu - System optimization - Fix integration errors related to Bedrock AI ChatBot 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ Fri - Write Lambda Function to integrate Bedrock into Front-end instead of Back-end - Research how to integrate Bedrock into the project\u0026rsquo;s Front-end 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Advanced NLP Mastery (BERT):\nUnderstood the Bidirectional Encoder Representations from Transformers (BERT) architecture. Successfully applied Multi-Task Training Strategies. Completed the lab on Fine-tuning BERT with custom datasets to improve model accuracy. Product Finalization:\nCompleted the User Interface (UI) and verified all core functionalities. Executed a full testing phase to identify and resolve bugs. Architectural Refactoring \u0026amp; Optimization:\nTransitioned the AI Chatbot integration from a monolithic backend approach to a Serverless architecture. Developed AWS Lambda functions to allow the Frontend to communicate directly with AWS Bedrock, improving latency and scalability. Optimized overall system performance based on team feedback. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Complete and formalize all project documentation (Internship Report, Proposal, and Workshop materials). Synchronize the technical architecture diagram with the actual implemented solution. Finalize the integration of all AWS services. Execute the final deployment of the project to the AWS Cloud and perform post-deployment testing. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Write Internship Report - Continue system optimization 17/11/2025 17/11/2025 Tue - Revise the architecture diagram to match project changes - Revise Proposal and Workshop materials - Team meeting to discuss the diagram and project completion status 18/11/2025 18/11/2025 Wed - Edit and finalize the final project 19/11/2025 19/11/2025 Thu - Integrate AWS services into the project 20/11/2025 20/11/2025 https://cloudjourney.awsstudygroup.com/ Fri - Check and debug errors after deploying the project to AWS 21/11/2025 21/11/2025 Week 11 Achievements: Documentation Completion:\nDrafted and refined the Internship Report. Updated the Project Proposal and Workshop content to reflect the final product. Architectural Synchronization:\nRevised the System Architecture Diagram to ensure 100% accuracy with the deployed services. Reached a final consensus with the team on the project structure. Deployment \u0026amp; Integration:\nSuccessfully integrated requisite AWS services (e.g., Bedrock, Lambda, Database) into the core application. Deployed the final project to the AWS environment. Conducted rigorous post-deployment testing and resolved integration bugs to ensure stability. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Complete and polish the Internship Report. Identify, debug, and resolve all remaining issues found during the deployment phase. Implement final system optimizations based on team consensus. Conduct a final internal review and user experience session for the semester project. Tasks to implement this week: Day Task Start Date End Date Resources Mon - Write Internship Report - Fix bugs detected during the project deployment process 24/11/2025 24/11/2025 Tue - Continue project optimization - Team meeting to discuss errors encountered during work and propose solutions together 25/11/2025 25/11/2025 Wed - Complete the system according to the solutions proposed by the group 26/11/2025 26/11/2025 Thu - Complete the system according to the solutions proposed by the group 27/11/2025 27/11/2025 Fri - Finalize the project - Meeting to experience and demo the product of this semester together 28/11/2025 28/11/2025 Week 12 Achievements: Documentation \u0026amp; Reporting:\nCompleted the drafting and editing of the Internship Report. Quality Assurance (QA) \u0026amp; Debugging:\nSuccessfully identified critical bugs arising from the initial deployment. Collaborated with the team to brainstorm and agree upon effective technical solutions. System Finalization:\nImplemented the agreed-upon solutions, resulting in a stable and optimized system. The system is now fully functional and ready for final submission. Product Completion:\nConducted a successful \u0026ldquo;End-of-Semester\u0026rdquo; product demo/experience session with the team, confirming that all features work as intended. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.4-databaseandstorage/5.4.3-create-ecr/","title":"Create AWS ECR","tags":[],"description":"","content":" Open the Amazon Elastic Container Registry In create console, fill in repository name Choose Mutable in Image tag mutability Then click Create "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" Blog 1 - How Volkswagen and AWS built a complete MLOps process for the Digital Production Platform This blog shares the success story of the collaboration between Volkswagen and AWS in building the Digital Production Platform (DPP). You will explore how they implemented a comprehensive MLOps (Machine Learning Operations) process to automate and standardize the development, deployment, and management of machine learning models. The article delves into the challenges within the industrial production environment, architectural solutions for scaling, and how this process helps Volkswagen accelerate innovation and optimize operational efficiency across its entire global supply chain.\nBlog 2 - Implement recovery testing to validate recovery with AWS Backup This blog emphasizes that data backup alone is not enough; you must ensure that data is recoverable when needed. The article provides a detailed guide on using the AWS Backup Restore Testing feature to automate the recovery testing process. You will learn how to set up periodic testing plans, validate data integrity after recovery, and optimize costs. This is a crucial resource for helping organizations meet strict compliance standards and ensure Disaster Recovery capabilities.\nBlog 3 - Improve PostgreSQL Performance using pgstattuple extension This blog is an in-depth technical guide on optimizing performance for PostgreSQL databases on Amazon RDS and Aurora. You will be introduced to the pgstattuple extension – a powerful tool for analyzing the physical storage level of the database. The article explains concepts such as \u0026ldquo;dead tuples\u0026rdquo; and \u0026ldquo;table bloat,\u0026rdquo; how to use pgstattuple to diagnose these issues, and best practices for cleanup and maintenance to keep the database running at peak performance.\n"},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.3-vpc/","title":"VPC","tags":[],"description":"","content":"Intro VPC (Virtual Private Cloud) is a logically isolated virtual private network space in AWS Cloud. It acts as your personal data center in the cloud, giving you complete control over the network environment.\nCore components: Route Table Subnets Internet Gateway Security Groups Create VPC Open the Amazon VPC console Choose Your VPCs, then click Create VPC In the create VPC console: Specify name of the Name tag: my-vpc-01 IPv4 CIDR : 10.0.0.0/16 Then click Create VPC Content Create Route Table \u0026amp; Internet Gateway Create Subnets Create Security Groups Create VPC Endpoints "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/4-eventparticipated/","title":"Attended Events","tags":[],"description":"","content":" During my internship, I participated in 3 events. Each event was a memorable experience filled with new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AWS Well-Architected Security Pillar\nTime: 09:00, November 29, 2025\nLocation: Level 26, Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: DevOps on AWS\nTime: 09:00, November 17, 2025\nLocation: Level 26, Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS AI/ML and Generative AI Workshop\nTime: 09:00, November 15, 2025\nLocation: Level 26, Bitexco Financial Tower, No. 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.4-databaseandstorage/","title":"Database And Storage","tags":[],"description":"","content":"Overview This project used three data storage services which are: AWS RDS(Relational Database Service) is a AWS service to config database and is located in Private Subnet AWS S3 (Simple Storage Service) is a AWS service to storage picture and video AWS ECR (Elastic Container Registry) is a AWS service to storage docker image Content Create RDS Create S3 Create ECR "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.5-computeandcontainer/","title":"Compute And Container","tags":[],"description":"","content":"IAM Role Create ecsTaskExecutionRole role to AWS ECS (Elastic Container Service) pull docker image and write logs from ECR repository Open the Amazon IAM In left navbar, choose Roles, then click Create role In create console, choose AWS service and select Use case is EC2 then click Next In Add permissions step, search AmazonECSTaskExecutionRolePolicy and select this Policy, then click Next In Name, review, and create, fill in Role name is ecsTaskExecutionRole , then click Create role Create FargateTaskRole role to allow Fargate access resource In left navbar, choose Roles, then click Create role In create console, choose AWS service and select Use case is EC2 then click Next In Add permissions step, search AmazonS3FullAccess, AmazonSESFullAccess and AmazonSNSFullAccess, then click Next Review and fill in Role name is FargateTaskRole, then click Create role ECS Cluster Create ECS Cluster is place to Fargate running Open the Amazon ECS In left navbar, choose Clusters, then click Create Cluster In create console, fill in Cluster name In Infrastructure, select Fargate only Then click Create Task definitions Create Task definitions is a container design In left navbar of ECS console, choose Task definitions, then click **Create new task definitions In create console, fill in Task definition family Select AWS Fargate in Launch type of Infrastructure requirements Choose Task role is FargateTaskRole Choose Task execution role is ecsTaskExecutionRole In Container, fill in name and URL of ECR, then Add Environment variable Scroll down to the bottom and click Create ECS Service Create ECS Service to run Task definitions In left navbar, choose Clusters, then click Cluster created In tab Sevices, click Create In create console, choose Task definition family Choose Task definition revision Fill in Service name In Compute options choose Capacity provider strategy In Capacity provider, select FARGATE In Platform version, select LATEST In Deployment configuration, Scheduling strategy is Replica and Desired tasks is 1 Then click Create "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Deploy ReGenZet Management System To AWS Overview ReGenZet is an enterprise-grade EV garage management platform. The objective of this workshop is to design and deploy a secure, cost-optimized, and highly automated cloud infrastructure on AWS to host ApexEV\u0026rsquo;s frontend, backend, storage, and serverless AI/ML functions.\nKey architectural principles:\nSecurity-first: least-privilege IAM, encrypted data at rest and in transit, network isolation and controlled service endpoints. Cost optimization: use managed services with pay-as-you-go models, right-sizing, and automated lifecycle policies for storage and compute. Automation \u0026amp; Observability: Infrastructure-as-Code, CI/CD pipelines, centralized logging, and automated monitoring/alarms. Core services used in this workshop:\nAWS ECS (Fargate) — run backend microservices without managing servers. AWS Amplify — host the frontend, provide CI/CD for web clients and manage hosting. Amazon RDS — managed relational database for transactional data. Amazon S3 — object storage for media, backups, and static assets. AWS Lambda — serverless functions for AI/ML processing pipelines, notifications and background tasks. This workshop contains hands-on modules covering the end-to-end stack and best practices for each layer.\nContent Workshop Overview Project Architecture VPC Of Project Database And Storage Compute And Container Create Load Balancing Create Amplify And API Gateway Intruct Deploy Code Frontend and Backend "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [First Cloud Journey / AWS Study Group] from September 15, 2025 to November 28, 2025, I had the opportunity to bridge the gap between academic theory and professional application in the field of Cloud Computing and Artificial Intelligence.\nI actively participated in the \u0026ldquo;Development of an Intelligent AI Chatbot using AWS Bedrock and Serverless Architecture\u0026rdquo; project. Through this intensive period, I significantly improved my skills in Cloud Infrastructure (AWS VPC, EC2, Lambda), Natural Language Processing (BERT, Transformers), and Full-stack System Integration.\nIn terms of work ethic, I consistently strived to master new technologies (such as GenAI and Serverless), complied with project timelines, and collaborated effectively with mentors and team members to optimize the system architecture.\nTo objectively reflect on my internship period, I evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Grasping AWS core services, NLP algorithms, and applying them to build a functional product. ✅ ☐ ☐ 2 Ability to learn Rapidly adapting to new tech stacks (AWS Bedrock, Hugging Face) within a short timeframe. ✅ ☐ ☐ 3 Proactiveness Proposing architectural changes (Migration to Serverless) to improve performance. ✅ ☐ ☐ 4 Sense of responsibility Committing to deadlines for the Proposal, Midterm Exam, and Final Deployment. ✅ ☐ ☐ 5 Discipline Adhering to strict schedules, reporting guidelines, and organizational rules. ☐ ✅ ☐ 6 Progressive mindset Receptive to feedback regarding Architecture Diagrams and UI/UX design. ✅ ☐ ☐ 7 Communication Articulating technical concepts clearly in weekly reports and team meetings. ☐ ✅ ☐ 8 Teamwork Coordinating with Front-end and Back-end members to resolve integration conflicts. ✅ ☐ ☐ 9 Professional conduct Maintaining a respectful and professional attitude towards mentors and peers. ✅ ☐ ☐ 10 Problem-solving skills Debugging integration errors and optimizing system latency during the testing phase. ☐ ✅ ☐ 11 Contribution to project/team Delivering a working AI module and finalizing the system deployment on AWS. ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period. ✅ ☐ ☐ Areas for Improvement Enhance Operational Discipline: I need to strengthen my adherence to strict organizational workflows and time management to avoid last-minute crunches during deployment phases. Deepen Problem-Solving Strategy: Move from \u0026ldquo;reactive debugging\u0026rdquo; to \u0026ldquo;proactive architectural design\u0026rdquo; to anticipate and prevent system errors before they occur. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.7-amplifyandapigateway/","title":"Create Amplify And API Gateway","tags":[],"description":"","content":"AWS Amplify Create Amplify to deploy Frontend Open the Amazon Amplify Click Create new app In create console, choose Gitlab, then click Next In step Add repository and branch, login with Gitlab, then choose Git Repository and branch need to deploy Then click Next Setting format with your type code in your Frontend, then click Next Review and click Save and deploy After this step wait about 3-5 minutes to deploy and you can access your app from internet API Gateway API Gateway is service to transfer HTTP and HTTPS between Amplify (Frontend) and Fargate (Backend) Open the Amazon API Gateway Click Create API Choose REST API and click Build In create console, choose API details is New API Fill in API name API endpoint type is Regional Security policy is SecurityPolicy_TLS13_1_2_2021_06 Then click Create API In left navbar, choose APIs and select API Gateway created In console API Gateway click Create Resource then fill in Resource name and click Create resource In console resource API Gateway, click Create method In Method details choose ANY, and Integration type is HTTP Proxy HTTP method select ANY, and Endpoint URL is Endpoints of ALB Then scroll down to the bottom and click Create method In resource proxy, create method In create console, choose Integration type is Mock and Method type is OPTIONS Then click Create method Now we finish setup API Gateway for project "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.6-createalb/","title":"Create AWS Load Balancers","tags":[],"description":"","content":" Open the Amazon EC2 In left navbar, choose Load Balancers, then click Create load balancer Choose Application Load Balancer In create console, fill in Load balancer name Scheme is Internet-facing, then select VPC In Availability Zones and subnets choose two AZ and select two subnet in public\\ Security groups select alb-sg Then scroll down to the bottom and click Create load balancer "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/5-workshop/5.8-instruct-deploy-be-fe/","title":"Instruct Deploy BackEnd And Frontend","tags":[],"description":"","content":"Deploy Frontend Instructure Deploy Frontend to Amplify Open terminal in your code from your compute Commit and Push to branch of Gitlab which define in Amplify Amplify auto CICD and deploy your Frontend Wait 3-5 minutes Deploy Backend Instructure Deploy Backend to Fargate Open the Amazon ECR In left navbar, choose Repository and select ECR repository created Open terminal in your code from your compute Write syntax \u0026ldquo;AWS configure\u0026rdquo; to setup access key and secret key of your AWS account After setup AWS account for CLI, comeback console of ECR Repository Click View push command, you will see syntax to push docker image to ECR repository Then copy and paste it to terminal of your Backend project Fisnish this step, AWS ECS Fargate will auto pull and run image have tag latest Finally,Congratulations for finish this workshop with deploy success project to AWS. "},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/7-feedback/","title":"Sharing &amp; Feedback","tags":[],"description":"","content":"General Assessment 1. Work Environment The environment at FCJ is truly dynamic and \u0026ldquo;tech-driven.\u0026rdquo; The atmosphere always encourages curiosity and innovation. I was very impressed by how everyone discusses architecture solutions – there is no gap between \u0026ldquo;bosses\u0026rdquo; and interns; everyone focuses solely on finding the optimal solution for the problem.\n2. Support from Mentor / Team Admin The Mentors act less like teachers and more like Senior Cloud Engineers guiding a junior. When I encountered errors (e.g., unable to SSH into an EC2 instance or RDS connection failures), the Mentor didn\u0026rsquo;t fix them for me immediately. Instead, they provided keywords and guided me on how to read logs in CloudWatch to debug it myself. Although this approach was challenging at first, it helped me understand the problems much more deeply. The Team Admin was also very enthusiastic in supporting AWS lab accounts so I wasn\u0026rsquo;t interrupted during practice.\n3. Suitability of Work to Major As an IT student, participating in FCJ was the perfect piece to turn \u0026ldquo;Computer Networks\u0026rdquo; and \u0026ldquo;Operating Systems\u0026rdquo; theories into reality. Abstract concepts like IP and Subnet masks became tangible through configuring VPCs on AWS. This is a solid foundation for my career path as a System/DevOps Engineer.\n4. Learning Opportunities \u0026amp; Skill Development This is the part I value the most. From someone vague about the Cloud, I have mastered the core AWS services:\nEC2 (Elastic Compute Cloud): Learned how to launch virtual servers, select appropriate instance types, and manage Security Groups. VPC (Virtual Private Cloud): Gained deep understanding of network architecture, dividing Private/Public Subnets, and configuring Route Tables and Internet Gateways. S3 (Simple Storage Service): Learned how to host static websites and manage bucket policy permissions. IAM (Identity and Access Management): Understood the importance of the \u0026ldquo;least privilege\u0026rdquo; principle when assigning permissions to users and roles. RDS (Relational Database Service): Learned how to deploy databases on the cloud and connect securely to the backend. 5. Culture \u0026amp; Team Spirit The \u0026ldquo;Share to learn\u0026rdquo; culture is very evident. Every week, the team holds small tech-talks to share new services or difficult case studies. Team spirit peaks when the whole group troubleshoots a deployment error together until late at night, but everyone is happy when the system runs successfully (Status check: 2/2 passed).\n6. Intern Policies / Benefits Besides the allowance, the most \u0026ldquo;profitable\u0026rdquo; thing for me was being granted access to the AWS Sandbox environment to experiment without fear of \u0026ldquo;bill shock.\u0026rdquo; Internal documentation resources and training workshops are also invaluable knowledge benefits.\nOther Questions What were you most satisfied with during your internship? It was the \u0026ldquo;aha!\u0026rdquo; moment when I successfully deployed a complete 3-tier architecture on AWS by myself, from the Load Balancer pointing to the Auto Scaling Group, down to the Database located in the Private Subnet. It proved that the knowledge I learned is practical and immediately applicable.\nWhat do you think the company needs to improve for future interns? I think the program could introduce Infrastructure as Code (IaC) labs like Terraform or CloudFormation a bit earlier. While clicking through the Console is intuitive, it becomes quite time-consuming when re-doing tasks multiple times.\nIf you were to introduce this to friends, would you recommend interning here? Why? Definitely YES. FCJ is the ideal environment for those who want to start a career in Cloud/DevOps. You won\u0026rsquo;t be assigned petty tasks; instead, you will be \u0026ldquo;immersed\u0026rdquo; in real projects and technologies.\nSuggestions \u0026amp; Wishes Do you have any suggestions to improve the internship experience? There should be more \u0026ldquo;Mock Interview\u0026rdquo; sessions or guidance for AWS certifications (like SAA-C03) towards the end of the internship so that interns feel more confident when applying for official positions.\nDo you want to continue this program in the future? I am very eager to continue accompanying FCJ, possibly as a Fresher or Junior Cloud Engineer, to continue conquering more advanced services like EKS or Serverless.\nOther feedback (feel free to share): Thank you to the FCJ team for being patient and creating conditions for me to make mistakes and learn from them. This has been a truly memorable journey!\n"},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://phucthichnghiccode.github.io/InternshipReport.github.io/tags/","title":"Tags","tags":[],"description":"","content":""}]