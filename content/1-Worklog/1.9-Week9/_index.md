---
title: "Week 9 Worklog"
date: "2025-11-05"
weight: 1
chapter: false
pre: " <b> 1.9. </b> "
---


### Week 9 Objectives:

* Deep dive into modern NLP architectures: Transformers, Attention Mechanisms, and Decoders.
* Explore Transfer Learning and Large Language Models (LLMs) like BERT, GPT, and T5.
* Implement Generative AI solutions using AWS Bedrock.
* Optimize the Final Project architecture and refine the Chatbot's performance/UI.

### Tasks to implement this week:
| Day | Task                                                                                                                                                                                                                                | Start Date | End Date   | Resources                                                  |
| --- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | ---------- | ---------------------------------------------------------- |
| Mon | - Learn about Transformers vs RNNs <br> - Learn about Scaled and Dot-Product Attention <br> - Learn about Masked Self Attention <br> - Learn about Transformer Decoder                                                              | 03/11/2025 | 03/11/2025 | <https://www.coursera.org/learn/attention-models-in-nlp/>  |
| Tue | - Complete practice code lab on Transformer Summarizer <br> - Optimize Chatbot and Interface                                                                                                                                        | 04/11/2025 | 04/11/2025 | <https://www.coursera.org/learn/attention-models-in-nlp/>  |
| Wed | - Meeting to change/optimize AWS services for the project <br> - Learn Transfer Learning in NLP <br> - Learn about Large Language Models: ELMo, GPT, BERT, T5                                                                       | 05/11/2025 | 05/11/2025 | <https://www.coursera.org/learn/attention-models-in-nlp/>  |
| Thu | - Learn about AWS Bedrock <br> - Experiment with AI Chatbot using AWS Bedrock                                                                                                                                                       | 06/11/2025 | 06/11/2025 |                                                            |
| Fri | - Search and generate data for the Chatbot created with AWS Bedrock <br> - Test Chatbot and fix processing errors                                                                                                                   | 07/11/2025 | 07/11/2025 |                                                            |


### Week 9 Achievements: 



* Mastered Advanced NLP Architectures:
  * Differentiated between RNNs and **Transformers**.
  * Understood core mechanisms: **Scaled Dot-Product Attention** and **Masked Self-Attention**.
  * Implemented a **Transformer Summarizer** via code labs.

* Understanding Large Language Models (LLMs): 
  * Gained insight into **Transfer Learning** strategies.
  * Explored the evolution of major models: **ELMo, GPT, BERT, and T5**.

* Applied Generative AI with AWS: 


  * Successfully integrated **AWS Bedrock** to upgrade the AI Chatbot capabilities.
  * Curated datasets for fine-tuning/context and resolved integration bugs.

* Project Optimization:
  * Refined the User Interface (UI) for better user experience.
  * Re-evaluated and optimized the AWS service stack for the final product.